{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ae7316",
   "metadata": {},
   "source": [
    "# Trainable ResNet-based CTâ€“MRI Fusion (No Ground Truth)\n",
    "\n",
    "This notebook implements a ResNet-based feature fusion approach for CT and MRI images, using pretrained ResNet for feature extraction and learning fusion in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a56e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e74c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for paired medical images\n",
    "class PairedMedicalFusionDataset(Dataset):\n",
    "    \"\"\"Paired dataset for registered CT & MRI images.\n",
    "    - root_ct, root_mri: directories with identical filenames\n",
    "    - resize: (H,W) or None\n",
    "    - file_exts: accepted extensions\n",
    "    \"\"\"\n",
    "    def __init__(self, root_ct: str, root_mri: str, resize: Tuple[int,int]=None,\n",
    "                 file_exts=(\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\")):\n",
    "        self.root_ct = root_ct\n",
    "        self.root_mri = root_mri\n",
    "        self.resize = resize\n",
    "\n",
    "        def index_dir(root):\n",
    "            paths = []\n",
    "            for ext in file_exts:\n",
    "                paths.extend(glob.glob(os.path.join(root, f\"**/*{ext}\"), recursive=True))\n",
    "            base = {os.path.splitext(os.path.relpath(p, root))[0].replace('\\\\','/') : p for p in paths}\n",
    "            return base\n",
    "\n",
    "        base_ct = index_dir(root_ct)\n",
    "        base_mr = index_dir(root_mri)\n",
    "        self.keys = sorted(list(set(base_ct.keys()) & set(base_mr.keys())))\n",
    "        if not self.keys:\n",
    "            raise RuntimeError(\"No paired files found. Ensure matching filenames between CT and MRI.\")\n",
    "        self.base_ct = base_ct\n",
    "        self.base_mr = base_mr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        key = self.keys[idx]\n",
    "        p_ct = self.base_ct[key]\n",
    "        p_mr = self.base_mr[key]\n",
    "\n",
    "        ct = cv2.imread(p_ct, cv2.IMREAD_GRAYSCALE)\n",
    "        mr = cv2.imread(p_mr, cv2.IMREAD_GRAYSCALE)\n",
    "        if ct is None or mr is None:\n",
    "            raise FileNotFoundError(p_ct if ct is None else p_mr)\n",
    "        if self.resize is not None:\n",
    "            H, W = self.resize\n",
    "            ct = cv2.resize(ct, (W, H), interpolation=cv2.INTER_AREA)\n",
    "            mr = cv2.resize(mr, (W, H), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        ct = ct.astype(np.float32)\n",
    "        mr = mr.astype(np.float32)\n",
    "        if ct.max() > 1.0: ct /= 255.0\n",
    "        if mr.max() > 1.0: mr /= 255.0\n",
    "\n",
    "        ct_t = torch.from_numpy(ct)[None, ...]   # (1,H,W)\n",
    "        mr_t = torch.from_numpy(mr)[None, ...]\n",
    "        return ct_t, mr_t, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss components\n",
    "class SSIM(nn.Module):\n",
    "    def __init__(self, window_size=11, C1=0.01**2, C2=0.03**2):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        gauss = cv2.getGaussianKernel(window_size, window_size/6)\n",
    "        gauss = gauss @ gauss.T\n",
    "        w = torch.from_numpy(gauss.astype(np.float32))[None, None]\n",
    "        self.register_buffer('window', w)\n",
    "\n",
    "    def _filt(self, x):\n",
    "        pad = self.window_size//2\n",
    "        window = self.window.to(x.device)\n",
    "        return F.conv2d(x, window, padding=pad, groups=x.size(1))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x,y: (B,1,H,W) in [0,1]\n",
    "        mu_x = self._filt(x)\n",
    "        mu_y = self._filt(y)\n",
    "        mu_x2, mu_y2, mu_xy = mu_x*mu_x, mu_y*mu_y, mu_x*mu_y\n",
    "        sigma_x2 = self._filt(x*x) - mu_x2\n",
    "        sigma_y2 = self._filt(y*y) - mu_y2\n",
    "        sigma_xy = self._filt(x*y) - mu_xy\n",
    "        ssim = ((2*mu_xy + self.C1)*(2*sigma_xy + self.C2)) / ((mu_x2 + mu_y2 + self.C1)*(sigma_x2 + sigma_y2 + self.C2) + 1e-8)\n",
    "        return ssim.mean()\n",
    "\n",
    "class GradientLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        kx = np.array([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=np.float32)\n",
    "        ky = np.array([[1,2,1],[0,0,0],[-1,-2,-1]], dtype=np.float32)\n",
    "        self.register_buffer('kx', torch.from_numpy(kx)[None, None])\n",
    "        self.register_buffer('ky', torch.from_numpy(ky)[None, None])\n",
    "\n",
    "    def forward(self, fused, ct, mr):\n",
    "        def grad(img):\n",
    "            gx = F.conv2d(img, self.kx, padding=1)\n",
    "            gy = F.conv2d(img, self.ky, padding=1)\n",
    "            return torch.sqrt(gx*gx + gy*gy + 1e-8)\n",
    "        gF = grad(fused)\n",
    "        gC = grad(ct)\n",
    "        gM = grad(mr)\n",
    "        gT = torch.max(gC, gM)\n",
    "        return F.l1_loss(gF, gT)\n",
    "\n",
    "class VGGPerceptual(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        self.slice1 = nn.Sequential(*[vgg[i] for i in range(4)])   # relu1_2\n",
    "        self.slice2 = nn.Sequential(*[vgg[i] for i in range(4,9)]) # relu2_2\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is single-channel by averaging across channels if needed,\n",
    "        # then convert to 3-channel RGB and apply ImageNet normalization.\n",
    "        if x.size(1) != 1:\n",
    "            x_gray = x.mean(dim=1, keepdim=True)\n",
    "        else:\n",
    "            x_gray = x\n",
    "        x3 = x_gray.repeat(1, 3, 1, 1)\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406], device=x.device)[None,:,None,None]\n",
    "        std  = torch.tensor([0.229, 0.224, 0.225], device=x.device)[None,:,None,None]\n",
    "        x3 = (x3 - mean) / std\n",
    "        f1 = self.slice1(x3)\n",
    "        f2 = self.slice2(f1)\n",
    "        return f1, f2\n",
    "\n",
    "def perceptual_loss(vgg: VGGPerceptual, fused, ct, mr):\n",
    "    fF1, fF2 = vgg(fused)\n",
    "    cF1, cF2 = vgg(ct)\n",
    "    mF1, mF2 = vgg(mr)\n",
    "    return 0.5*(F.l1_loss(fF1, cF1) + F.l1_loss(fF1, mF1) + F.l1_loss(fF2, cF2) + F.l1_loss(fF2, mF2))\n",
    "\n",
    "class FusionLoss(nn.Module):\n",
    "    def __init__(self, device, w_ssim=1.0, w_grad=1.0, w_perc=0.1):\n",
    "        super().__init__()\n",
    "        self.ssim = SSIM()\n",
    "        self.grad = GradientLoss()\n",
    "        self.vgg = VGGPerceptual(device)\n",
    "        self.w_ssim = w_ssim\n",
    "        self.w_grad = w_grad\n",
    "        self.w_perc = w_perc\n",
    "    def forward(self, fused, ct, mr):\n",
    "        l_ssim = 0.5*(1.0 - self.ssim(fused, ct)) + 0.5*(1.0 - self.ssim(fused, mr))\n",
    "        l_grad = self.grad(fused, ct, mr)\n",
    "        l_perc = perceptual_loss(self.vgg, fused, ct, mr)\n",
    "        total = self.w_ssim*l_ssim + self.w_grad*l_grad + self.w_perc*l_perc\n",
    "        return total, {\"ssim\": l_ssim.item(), \"grad\": l_grad.item(), \"perc\": l_perc.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c826872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-based Fusion Network\n",
    "class ResNetFusionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.fusion1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.fusion2 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.fusion3 = nn.Sequential(\n",
    "            nn.Conv2d(2048, 1024, 3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, 2, stride=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 256, 2, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 128, 2, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 1, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = x.repeat(1, 3, 1, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        f1 = self.layer1(x)\n",
    "        f2 = self.layer2(f1)\n",
    "        f3 = self.layer3(f2)\n",
    "        return f1, f2, f3\n",
    "\n",
    "    def forward(self, ct, mr):\n",
    "        ct_f1, ct_f2, ct_f3 = self.extract_features(ct)\n",
    "        mr_f1, mr_f2, mr_f3 = self.extract_features(mr)\n",
    "        fused_f1 = self.fusion1(torch.cat([ct_f1, mr_f1], dim=1))\n",
    "        fused_f2 = self.fusion2(torch.cat([ct_f2, mr_f2], dim=1))\n",
    "        fused_f3 = self.fusion3(torch.cat([ct_f3, mr_f3], dim=1))\n",
    "        x = self.up3(fused_f3)\n",
    "        x = torch.cat([x, fused_f2], dim=1)\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, fused_f1], dim=1)\n",
    "        x = self.up1(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ee0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config class\n",
    "@dataclass\n",
    "class Config:\n",
    "    ct_dir: str = 'Harvard-Medical-Image-Fusion-Datasets/MyDatasets/CT-MRI/train/CT'\n",
    "    mri_dir: str = 'Harvard-Medical-Image-Fusion-Datasets/MyDatasets/CT-MRI/train/MRI'\n",
    "    save_dir: str = 'checkpoints_resnet'\n",
    "    resize: Tuple[int,int] = (256, 256)\n",
    "    batch_size: int = 4\n",
    "    epochs: int = 5\n",
    "    lr: float = 2e-4\n",
    "    val_split: float = 0.1\n",
    "    w_ssim: float = 1.0\n",
    "    w_grad: float = 0.5\n",
    "    w_perc: float = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d31faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "cfg = Config()\n",
    "print(cfg)\n",
    "model = ResNetFusionNet().to(device)\n",
    "for param in model.conv1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.bn1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.layer1.parameters():\n",
    "    param.requires_grad = False\n",
    "loss_fn = FusionLoss(device, w_ssim=cfg.w_ssim, w_grad=cfg.w_grad, w_perc=cfg.w_perc).to(device)\n",
    "dataset = PairedMedicalFusionDataset(cfg.ct_dir, cfg.mri_dir, resize=cfg.resize)\n",
    "train_size = int((1 - cfg.val_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=cfg.batch_size, shuffle=False, num_workers=2)\n",
    "print(f\"Dataset size: {len(dataset)}, Train: {len(train_set)}, Val: {len(val_set)}\")\n",
    "opt = torch.optim.Adam([\n",
    "    {'params': model.layer2.parameters(), 'lr': cfg.lr * 0.1},\n",
    "    {'params': model.layer3.parameters(), 'lr': cfg.lr * 0.1},\n",
    "    {'params': model.fusion1.parameters(), 'lr': cfg.lr},\n",
    "    {'params': model.fusion2.parameters(), 'lr': cfg.lr},\n",
    "    {'params': model.fusion3.parameters(), 'lr': cfg.lr},\n",
    "    {'params': model.up3.parameters(), 'lr': cfg.lr},\n",
    "    {'params': model.up2.parameters(), 'lr': cfg.lr},\n",
    "    {'params': model.up1.parameters(), 'lr': cfg.lr},\n",
    "    {'params': model.final.parameters(), 'lr': cfg.lr}\n",
    "], lr=cfg.lr)\n",
    "scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "os.makedirs(cfg.save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val = float('inf')\n",
    "for epoch in range(1, cfg.epochs+1):\n",
    "    model.train()\n",
    "    tr_running = 0.0\n",
    "    for ct, mr, _ in train_loader:\n",
    "        ct = ct.to(device)\n",
    "        mr = mr.to(device)\n",
    "        fused = model(ct, mr)\n",
    "        loss, parts = loss_fn(fused, ct, mr)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        tr_running += loss.item() * ct.size(0)\n",
    "    tr_loss = tr_running / len(train_set)\n",
    "    model.eval()\n",
    "    va_running = 0.0\n",
    "    with torch.no_grad():\n",
    "        for ct, mr, _ in val_loader:\n",
    "            ct = ct.to(device)\n",
    "            mr = mr.to(device)\n",
    "            fused = model(ct, mr)\n",
    "            loss, parts = loss_fn(fused, ct, mr)\n",
    "            va_running += loss.item() * ct.size(0)\n",
    "    va_loss = va_running / len(val_set)\n",
    "    scheduler.step(va_loss)\n",
    "    print(f\"Epoch {epoch:03d} | train {tr_loss:.4f} | val {va_loss:.4f}\")\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        ckpt_path = os.path.join(cfg.save_dir, 'resnet_fusion_best.pt')\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'opt': opt.state_dict(),\n",
    "            'cfg': cfg.__dict__\n",
    "        }, ckpt_path)\n",
    "        print('Saved:', ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def fuse_with_resnet(ct_path, mr_path, model, device, resize=None):\n",
    "    ct = cv2.imread(ct_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mr = cv2.imread(mr_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if resize is not None:\n",
    "        ct = cv2.resize(ct, resize, interpolation=cv2.INTER_AREA)\n",
    "        mr = cv2.resize(mr, resize, interpolation=cv2.INTER_AREA)\n",
    "    ct = ct.astype(np.float32) / 255.0\n",
    "    mr = mr.astype(np.float32) / 255.0\n",
    "    ct_t = torch.from_numpy(ct)[None, None, ...].to(device)\n",
    "    mr_t = torch.from_numpy(mr)[None, None, ...].to(device)\n",
    "    with torch.no_grad():\n",
    "        fused = model(ct_t, mr_t)\n",
    "    return fused.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "best_model = ResNetFusionNet().to(device)\n",
    "checkpoint = torch.load('checkpoints_resnet/resnet_fusion_best.pt', weights_only=False)\n",
    "best_model.load_state_dict(checkpoint['model'])\n",
    "best_model.eval()\n",
    "sample_ct_files = glob.glob(os.path.join(cfg.ct_dir, '*.png'))\n",
    "sample_mri_files = glob.glob(os.path.join(cfg.mri_dir, '*.png'))\n",
    "if sample_ct_files and sample_mri_files:\n",
    "    ct_example = sample_ct_files[0]\n",
    "    ct_basename = os.path.splitext(os.path.basename(ct_example))[0]\n",
    "    mr_example = os.path.join(cfg.mri_dir, f'{ct_basename}.png')\n",
    "    if os.path.exists(mr_example):\n",
    "        print(f'Using example pair: {ct_example} and {mr_example}')\n",
    "        fused_image = fuse_with_resnet(ct_example, mr_example, best_model, device, cfg.resize)\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(131)\n",
    "        ct_img = cv2.imread(ct_example, cv2.IMREAD_GRAYSCALE)\n",
    "        plt.imshow(ct_img, cmap='gray')\n",
    "        plt.title('CT Image')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(132)\n",
    "        mr_img = cv2.imread(mr_example, cv2.IMREAD_GRAYSCALE)\n",
    "        plt.imshow(mr_img, cmap='gray')\n",
    "        plt.title('MRI Image')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(fused_image, cmap='gray')\n",
    "        plt.title('Fused Image (ResNet)')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(cfg.save_dir, 'fusion_example.png'), dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f'Fusion example saved to {cfg.save_dir}/fusion_example.png')\n",
    "    else:\n",
    "        print(f'No matching MRI file found for {ct_example}')\n",
    "else:\n",
    "    print('No sample images found in the dataset directories')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
