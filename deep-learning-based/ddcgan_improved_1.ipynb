{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e445558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Work/conda/installation/envs/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/libtorch_1745854776362/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded. PyTorch: 2.5.1 CUDA: False\n",
      "Defining models...\n",
      "Model definitions done.\n",
      "Defining CT-MRI Dataset class...\n",
      "Defining trainer and utilities...\n",
      "Initializing trainer... device= cpu\n",
      "Dataset found 3208 pairs under ../Dataset/train\n",
      "Starting training: 3 epochs, dataset size: 3208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/201 [00:06<?, ?it/s, G=10.3268, D=1.1427, Rec=1.0953, Grad=0.1309, SSIM=0.9966]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 505\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m DDcGANTrainer(\n\u001b[1;32m    499\u001b[0m         dataset_path\u001b[38;5;241m=\u001b[39mDATASET_PATH,\n\u001b[1;32m    500\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m         device\u001b[38;5;241m=\u001b[39mDEVICE\n\u001b[1;32m    504\u001b[0m     )\n\u001b[0;32m--> 505\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# demonstration of inference on a random pair (even if untrained)\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdemo_inference\u001b[39m(checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "Cell \u001b[0;32mIn[1], line 467\u001b[0m, in \u001b[0;36mDDcGANTrainer.train\u001b[0;34m(self, num_epochs, save_interval)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training:\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_epochs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs, dataset size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset))\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 467\u001b[0m     g_loss, d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ep \u001b[38;5;241m%\u001b[39m save_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(ep)\n",
      "Cell \u001b[0;32mIn[1], line 403\u001b[0m, in \u001b[0;36mDDcGANTrainer.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# save sample occasionally\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_sample_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mct_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmri_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfused_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m avg_g_loss \u001b[38;5;241m=\u001b[39m avg_g_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, n)\n\u001b[1;32m    406\u001b[0m avg_d_loss \u001b[38;5;241m=\u001b[39m avg_d_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, n)\n",
      "Cell \u001b[0;32mIn[1], line 424\u001b[0m, in \u001b[0;36mDDcGANTrainer.save_sample_images\u001b[0;34m(self, ct_imgs, mri_imgs, fused_imgs, epoch, batch_idx)\u001b[0m\n\u001b[1;32m    422\u001b[0m ct_np \u001b[38;5;241m=\u001b[39m ((ct_sample \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    423\u001b[0m mri_np \u001b[38;5;241m=\u001b[39m ((mri_sample \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 424\u001b[0m fused_np \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfused_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    427\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(ct_np, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m); axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCT\u001b[39m\u001b[38;5;124m'\u001b[39m); axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# ddcgan_fusion_fixed.py\n",
    "# Full updated codebase (single-file). Replace your old script with this or copy into a notebook cell.\n",
    "\n",
    "# %%\n",
    "# Cell 1: Import libraries & set seed\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import kornia\n",
    "\n",
    "# reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"Libraries loaded. PyTorch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "# %%\n",
    "# Cell 2: Models (Encoder/Decoder/Generator/Discriminators)\n",
    "print(\"Defining models...\")\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, kernel=3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel, stride=stride, padding=kernel//2),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=2, base_channels=48):\n",
    "        super().__init__()\n",
    "        self.conv1 = Block(in_channels, base_channels, 1)      # preserve size\n",
    "        self.conv2 = Block(base_channels, base_channels, 2)   # downsample\n",
    "        self.conv3 = Block(base_channels, base_channels, 1)\n",
    "        self.conv4 = Block(base_channels, base_channels, 2)   # downsample\n",
    "        self.conv5 = Block(base_channels, base_channels, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return x  # shape: (B, C, H/4, W/4)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=48, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, 3, 1, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(16, out_channels, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=2, out_channels=1, base_channels=48):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_channels=in_channels, base_channels=base_channels)\n",
    "        self.decoder = Decoder(in_channels=base_channels, out_channels=out_channels)\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        out = self.decoder(feat)\n",
    "        return out, feat  # return both fused image and encoder features\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    # image-level discriminator\n",
    "    def __init__(self, in_channels=1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class FeatureDiscriminator(nn.Module):\n",
    "    # discriminator working on encoder feature maps\n",
    "    def __init__(self, feat_channels=48):\n",
    "        super().__init__()\n",
    "        # small conv net to judge feature maps\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(feat_channels, feat_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(feat_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feat_channels, 1)\n",
    "        )\n",
    "    def forward(self, feat):\n",
    "        return self.model(feat)\n",
    "\n",
    "print(\"Model definitions done.\")\n",
    "\n",
    "# %%\n",
    "# Cell 3: Dataset\n",
    "print(\"Defining CT-MRI Dataset class...\")\n",
    "\n",
    "class CTMRIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, img_size=(256,256)):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.image_pairs = []\n",
    "\n",
    "        ct_dir = self.root_dir / 'CT'\n",
    "        mri_dir = self.root_dir / 'MRI'\n",
    "\n",
    "        if ct_dir.exists() and mri_dir.exists():\n",
    "            ct_files = sorted(list(ct_dir.glob('*.png')) + list(ct_dir.glob('*.jpg')) + list(ct_dir.glob('*.jpeg')))\n",
    "            for ct in ct_files:\n",
    "                mri = mri_dir / ct.name\n",
    "                if mri.exists():\n",
    "                    self.image_pairs.append((str(ct), str(mri)))\n",
    "                else:\n",
    "                    # try same-stem with different extension if needed\n",
    "                    stems = list(mri_dir.glob(ct.stem + '.*'))\n",
    "                    if stems:\n",
    "                        self.image_pairs.append((str(ct), str(stems[0])))\n",
    "        else:\n",
    "            print(\"Warning: CT or MRI directory missing:\", ct_dir, mri_dir)\n",
    "\n",
    "        print(f\"Dataset found {len(self.image_pairs)} pairs under {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ct_path, mri_path = self.image_pairs[idx]\n",
    "        try:\n",
    "            ct_img = Image.open(ct_path).convert('L')  # grayscale\n",
    "            mri_img = Image.open(mri_path).convert('L')\n",
    "\n",
    "            ct_img = ct_img.resize(self.img_size, Image.Resampling.BILINEAR)\n",
    "            mri_img = mri_img.resize(self.img_size, Image.Resampling.BILINEAR)\n",
    "\n",
    "            if self.transform:\n",
    "                ct_t = self.transform(ct_img)\n",
    "                mri_t = self.transform(mri_img)\n",
    "            else:\n",
    "                ct_t = transforms.ToTensor()(ct_img)\n",
    "                mri_t = transforms.ToTensor()(mri_img)\n",
    "                ct_t = ct_t * 2.0 - 1.0\n",
    "                mri_t = mri_t * 2.0 - 1.0\n",
    "\n",
    "            return ct_t, mri_t\n",
    "        except Exception as e:\n",
    "            print(\"Error loading pair:\", ct_path, mri_path, e)\n",
    "            dummy = torch.rand(1, self.img_size[0], self.img_size[1]) * 2 - 1\n",
    "            return dummy, dummy\n",
    "\n",
    "# %%\n",
    "# Cell 4: Utilities and Trainer (major updates)\n",
    "print(\"Defining trainer and utilities...\")\n",
    "\n",
    "# Paths and directories\n",
    "RESULTS_DIR = \"results/ddcgan_fusion_fixed\"\n",
    "SAMPLES_DIR = os.path.join(RESULTS_DIR, \"samples\")\n",
    "PLOTS_DIR = os.path.join(RESULTS_DIR, \"plots\")\n",
    "CHECKPOINTS_DIR = \"checkpoints/intermediate/ddcgan_fusion_fixed\"\n",
    "FINAL_MODELS_DIR = \"checkpoints/final/ddcgan_fusion_fixed\"\n",
    "os.makedirs(SAMPLES_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_MODELS_DIR, exist_ok=True)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if getattr(m, \"bias\", None) is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def gradient_loss(gen_img, ref_img):\n",
    "    # Expect images in [-1,1], shape (B,1,H,W)\n",
    "    gen_grad = kornia.filters.sobel(gen_img)\n",
    "    ref_grad = kornia.filters.sobel(ref_img)\n",
    "    return torch.nn.functional.l1_loss(gen_grad, ref_grad)\n",
    "\n",
    "class DDcGANTrainer:\n",
    "    def __init__(self, dataset_path, batch_size=8, lr=2e-4, img_size=(256,256), device='cuda'):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "\n",
    "        print(\"Initializing trainer...\", \"device=\", self.device)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])  # [-1,1]\n",
    "        ])\n",
    "\n",
    "        self.dataset = CTMRIDataset(dataset_path, transform=transform, img_size=img_size)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "        # Models\n",
    "        self.generator = Generator(in_channels=2, out_channels=1).to(self.device)\n",
    "        self.disc_img = Discriminator(in_channels=1).to(self.device)       # image-level discriminator\n",
    "        self.disc_feat = FeatureDiscriminator(feat_channels=48).to(self.device)  # feature-level\n",
    "\n",
    "        self.generator.apply(weights_init)\n",
    "        self.disc_img.apply(weights_init)\n",
    "        self.disc_feat.apply(weights_init)\n",
    "\n",
    "        # Opts\n",
    "        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        self.d_img_optimizer = optim.Adam(self.disc_img.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        self.d_feat_optimizer = optim.Adam(self.disc_feat.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "        # Loss functions\n",
    "        self.adversarial_loss = nn.MSELoss()   # used for discriminator outputs\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        # use kornia SSIM\n",
    "        self.ssim_fn = kornia.losses.SSIMLoss(window_size=11, reduction='mean')\n",
    "\n",
    "        # weights (tuned to encourage both modalities)\n",
    "        self.w_recon = 5.0\n",
    "        self.w_grad = 5.0\n",
    "        self.w_ssim = 2.0\n",
    "        self.w_feat_adv = 1.0\n",
    "\n",
    "        # history\n",
    "        self.g_losses = []\n",
    "        self.d_losses = []\n",
    "        self.epoch_times = []\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.generator.train()\n",
    "        self.disc_img.train()\n",
    "        self.disc_feat.train()\n",
    "\n",
    "        start = time.time()\n",
    "        pbar = tqdm(self.dataloader, desc=f\"Epoch {epoch}\")\n",
    "        avg_g_loss = 0.0\n",
    "        avg_d_loss = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for i, (ct_imgs, mri_imgs) in enumerate(pbar):\n",
    "            n += 1\n",
    "            ct_imgs = ct_imgs.to(self.device)   # (B,1,H,W)\n",
    "            mri_imgs = mri_imgs.to(self.device)\n",
    "\n",
    "            # build generator input by concatenating CT and MRI (channel dim)\n",
    "            input_imgs = torch.cat([ct_imgs, mri_imgs], dim=1)  # (B,2,H,W)\n",
    "\n",
    "            batch_size = ct_imgs.size(0)\n",
    "            real_label = torch.ones(batch_size, 1, device=self.device)\n",
    "            fake_label = torch.zeros(batch_size, 1, device=self.device)\n",
    "\n",
    "            # ---------------------\n",
    "            # Train image discriminator (real from CT/MRI mix, fake from generator)\n",
    "            # ---------------------\n",
    "            self.d_img_optimizer.zero_grad()\n",
    "\n",
    "            # real fused samples: randomly choose CT or MRI — forces discriminator to accept both modality styles\n",
    "            # this makes discriminator less biased to one modality\n",
    "            if random.random() < 0.5:\n",
    "                real_fused = ct_imgs\n",
    "            else:\n",
    "                real_fused = mri_imgs\n",
    "\n",
    "            # generate fake fused images (detach -> for discriminator)\n",
    "            with torch.no_grad():\n",
    "                fake_fused, _ = self.generator(input_imgs)\n",
    "\n",
    "            real_pred = self.disc_img(real_fused)\n",
    "            fake_pred = self.disc_img(fake_fused.detach())\n",
    "\n",
    "            d_real_loss = self.adversarial_loss(real_pred, real_label)\n",
    "            d_fake_loss = self.adversarial_loss(fake_pred, fake_label)\n",
    "            d_img_loss = 0.5 * (d_real_loss + d_fake_loss)\n",
    "            d_img_loss.backward()\n",
    "            self.d_img_optimizer.step()\n",
    "\n",
    "            # ---------------------\n",
    "            # Train feature discriminator (on encoder feature maps)\n",
    "            # ---------------------\n",
    "            self.d_feat_optimizer.zero_grad()\n",
    "            # get encoder features for real (we compute encoder features for CT and MRI separately)\n",
    "            with torch.no_grad():\n",
    "                # encoder expects 2-channel input, create a \"pseudo\" paired input:\n",
    "                ct_pair = torch.cat([ct_imgs, ct_imgs], dim=1)   # (B,2,H,W)\n",
    "                mri_pair = torch.cat([mri_imgs, mri_imgs], dim=1)\n",
    "                ct_feat = self.generator.encoder(ct_pair)   # (B,C,H',W')\n",
    "                mri_feat = self.generator.encoder(mri_pair)\n",
    "            # sample features: pick CT or MRI features randomly as \"real\" features\n",
    "            real_feat = ct_feat if random.random() < 0.5 else mri_feat\n",
    "            # fake features are features from generator on mixed input\n",
    "            with torch.no_grad():\n",
    "                _, fake_feat = self.generator(input_imgs)\n",
    "            real_f_pred = self.disc_feat(real_feat)\n",
    "            fake_f_pred = self.disc_feat(fake_feat.detach())\n",
    "            d_feat_real_loss = self.adversarial_loss(real_f_pred, real_label)\n",
    "            d_feat_fake_loss = self.adversarial_loss(fake_f_pred, fake_label)\n",
    "            d_feat_loss = 0.5 * (d_feat_real_loss + d_feat_fake_loss)\n",
    "            d_feat_loss.backward()\n",
    "            self.d_feat_optimizer.step()\n",
    "\n",
    "            d_total = d_img_loss + d_feat_loss\n",
    "\n",
    "            # ---------------------\n",
    "            # Train generator (adversarial + reconstruction + gradient + ssim + feat adv)\n",
    "            # ---------------------\n",
    "            self.g_optimizer.zero_grad()\n",
    "            fused_imgs, fused_feat = self.generator(input_imgs)\n",
    "\n",
    "            # adversarial (image-level)\n",
    "            pred_img = self.disc_img(fused_imgs)\n",
    "            g_adv_img = self.adversarial_loss(pred_img, real_label)\n",
    "\n",
    "            # adversarial (feature-level) - try to fool the feature discriminator\n",
    "            pred_feat = self.disc_feat(fused_feat)\n",
    "            g_adv_feat = self.adversarial_loss(pred_feat, real_label)\n",
    "\n",
    "            # reconstruction L1 to both CT and MRI (equal weight)\n",
    "            rec_loss = self.l1_loss(fused_imgs, ct_imgs) + self.l1_loss(fused_imgs, mri_imgs)\n",
    "\n",
    "            # gradient preservation for both modalities\n",
    "            grad_loss = gradient_loss(fused_imgs, ct_imgs) + gradient_loss(fused_imgs, mri_imgs)\n",
    "\n",
    "            # SSIM preservation for both\n",
    "            ssim_loss = self.ssim_fn(fused_imgs, ct_imgs) + self.ssim_fn(fused_imgs, mri_imgs)\n",
    "\n",
    "            g_loss = g_adv_img + self.w_feat_adv * g_adv_feat + \\\n",
    "                     self.w_recon * rec_loss + \\\n",
    "                     self.w_grad * grad_loss + \\\n",
    "                     self.w_ssim * ssim_loss\n",
    "\n",
    "            g_loss.backward()\n",
    "            self.g_optimizer.step()\n",
    "\n",
    "            avg_g_loss += g_loss.item()\n",
    "            avg_d_loss += d_total.item()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'G': f'{g_loss.item():.4f}',\n",
    "                'D': f'{d_total.item():.4f}',\n",
    "                'Rec': f'{rec_loss.item():.4f}',\n",
    "                'Grad': f'{grad_loss.item():.4f}',\n",
    "                'SSIM': f'{ssim_loss.item():.4f}'\n",
    "            })\n",
    "\n",
    "            # save sample occasionally\n",
    "            if i % 200 == 0:\n",
    "                self.save_sample_images(ct_imgs, mri_imgs, fused_imgs, epoch, i)\n",
    "\n",
    "        avg_g_loss = avg_g_loss / max(1, n)\n",
    "        avg_d_loss = avg_d_loss / max(1, n)\n",
    "        elapsed = time.time() - start\n",
    "        self.g_losses.append(avg_g_loss)\n",
    "        self.d_losses.append(avg_d_loss)\n",
    "        self.epoch_times.append(elapsed)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch} done — G_loss: {avg_g_loss:.4f}, D_loss: {avg_d_loss:.4f}, time: {elapsed:.1f}s\")\n",
    "        return avg_g_loss, avg_d_loss\n",
    "\n",
    "    def save_sample_images(self, ct_imgs, mri_imgs, fused_imgs, epoch, batch_idx):\n",
    "        os.makedirs(SAMPLES_DIR, exist_ok=True)\n",
    "        ct_sample = ct_imgs[0].cpu()\n",
    "        mri_sample = mri_imgs[0].cpu()\n",
    "        fused_sample = fused_imgs[0].cpu()\n",
    "\n",
    "        # denormalize\n",
    "        ct_np = ((ct_sample + 1.0) / 2.0).squeeze().numpy()\n",
    "        mri_np = ((mri_sample + 1.0) / 2.0).squeeze().numpy()\n",
    "        fused_np = ((fused_sample + 1.0) / 2.0).squeeze().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(1,3, figsize=(12,4))\n",
    "        axes[0].imshow(ct_np, cmap='gray'); axes[0].set_title('CT'); axes[0].axis('off')\n",
    "        axes[1].imshow(mri_np, cmap='gray'); axes[1].set_title('MRI'); axes[1].axis('off')\n",
    "        axes[2].imshow(fused_np, cmap='gray'); axes[2].set_title('Fused'); axes[2].axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SAMPLES_DIR, f'epoch{epoch}_batch{batch_idx}.png'), dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self, epoch, path=CHECKPOINTS_DIR):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        ckpt = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': self.generator.state_dict(),\n",
    "            'disc_img_state_dict': self.disc_img.state_dict(),\n",
    "            'disc_feat_state_dict': self.disc_feat.state_dict(),\n",
    "            'g_optimizer': self.g_optimizer.state_dict(),\n",
    "            'd_img_optimizer': self.d_img_optimizer.state_dict(),\n",
    "            'd_feat_optimizer': self.d_feat_optimizer.state_dict(),\n",
    "            'g_losses': self.g_losses,\n",
    "            'd_losses': self.d_losses,\n",
    "            'epoch_times': self.epoch_times\n",
    "        }\n",
    "        torch.save(ckpt, os.path.join(path, f'ddcgan_fixed_epoch_{epoch}.pth'))\n",
    "        print(\"Saved checkpoint at epoch\", epoch)\n",
    "\n",
    "    def load_model(self, ckpt_path):\n",
    "        ckpt = torch.load(ckpt_path, map_location=self.device)\n",
    "        self.generator.load_state_dict(ckpt['generator_state_dict'])\n",
    "        self.disc_img.load_state_dict(ckpt['disc_img_state_dict'])\n",
    "        self.disc_feat.load_state_dict(ckpt['disc_feat_state_dict'])\n",
    "        self.g_optimizer.load_state_dict(ckpt['g_optimizer'])\n",
    "        self.d_img_optimizer.load_state_dict(ckpt['d_img_optimizer'])\n",
    "        self.d_feat_optimizer.load_state_dict(ckpt['d_feat_optimizer'])\n",
    "        self.g_losses = ckpt.get('g_losses', [])\n",
    "        self.d_losses = ckpt.get('d_losses', [])\n",
    "        print(\"Loaded checkpoint:\", ckpt_path)\n",
    "        return ckpt.get('epoch', 0)\n",
    "\n",
    "    def train(self, num_epochs=50, save_interval=5):\n",
    "        print(\"Starting training:\", num_epochs, \"epochs, dataset size:\", len(self.dataset))\n",
    "        for ep in range(1, num_epochs+1):\n",
    "            g_loss, d_loss = self.train_epoch(ep)\n",
    "            if ep % save_interval == 0:\n",
    "                self.save_model(ep)\n",
    "        # final save\n",
    "        self.save_model(num_epochs, path=FINAL_MODELS_DIR)\n",
    "        self.plot_training_metrics()\n",
    "\n",
    "    def plot_training_metrics(self):\n",
    "        os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "        epochs = range(1, len(self.g_losses)+1)\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(epochs, self.g_losses, label='G_loss')\n",
    "        plt.plot(epochs, self.d_losses, label='D_loss')\n",
    "        plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.title('Training losses')\n",
    "        plt.savefig(os.path.join(PLOTS_DIR, 'losses.png'), dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "# %%\n",
    "# Cell 5: Config & run (change DATASET_PATH as appropriate)\n",
    "if __name__ == \"__main__\":\n",
    "    DATASET_PATH = \"../Dataset/train\"   # <-- set to your dataset location\n",
    "    BATCH_SIZE = 16\n",
    "    LR = 2e-4\n",
    "    NUM_EPOCHS = 3\n",
    "    IMG_SIZE = (256,256)\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        print(\"Dataset path not found:\", DATASET_PATH)\n",
    "        print(\"Please set DATASET_PATH to your dataset folder containing CT/ and MRI/ subfolders.\")\n",
    "    else:\n",
    "        trainer = DDcGANTrainer(\n",
    "            dataset_path=DATASET_PATH,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            lr=LR,\n",
    "            img_size=IMG_SIZE,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        trainer.train(NUM_EPOCHS, save_interval=10)\n",
    "\n",
    "    # demonstration of inference on a random pair (even if untrained)\n",
    "    def demo_inference(checkpoint=None):\n",
    "        device = torch.device(DEVICE if torch.cuda.is_available() else 'cpu')\n",
    "        gen = Generator(in_channels=2).to(device)\n",
    "        if checkpoint and os.path.exists(checkpoint):\n",
    "            ck = torch.load(checkpoint, map_location=device)\n",
    "            gen.load_state_dict(ck['generator_state_dict'])\n",
    "            print(\"Loaded generator from\", checkpoint)\n",
    "        gen.eval()\n",
    "\n",
    "        # pick a random pair from dataset or dummy\n",
    "        if os.path.exists(DATASET_PATH):\n",
    "            ds = CTMRIDataset(DATASET_PATH, transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5],[0.5])\n",
    "            ]), img_size=IMG_SIZE)\n",
    "            if len(ds) > 0:\n",
    "                ct, mri = ds[random.randint(0, len(ds)-1)]\n",
    "                ct = ct.unsqueeze(0).to(device)\n",
    "                mri = mri.unsqueeze(0).to(device)\n",
    "            else:\n",
    "                ct = torch.rand(1,1,IMG_SIZE[0],IMG_SIZE[1]).to(device)*2-1\n",
    "                mri = torch.rand(1,1,IMG_SIZE[0],IMG_SIZE[1]).to(device)*2-1\n",
    "        else:\n",
    "            ct = torch.rand(1,1,IMG_SIZE[0],IMG_SIZE[1]).to(device)*2-1\n",
    "            mri = torch.rand(1,1,IMG_SIZE[0],IMG_SIZE[1]).to(device)*2-1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fused, _ = gen(torch.cat([ct, mri], dim=1))\n",
    "        # denorm\n",
    "        def denorm(x):\n",
    "            x = x.detach().cpu().squeeze()\n",
    "            x = (x + 1.0) / 2.0\n",
    "            x = x.clamp(0,1).numpy()\n",
    "            return x\n",
    "        ct_np = denorm(ct)\n",
    "        mri_np = denorm(mri)\n",
    "        fused_np = denorm(fused)\n",
    "\n",
    "        fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "        ax[0].imshow(ct_np, cmap='gray'); ax[0].set_title('CT'); ax[0].axis('off')\n",
    "        ax[1].imshow(mri_np, cmap='gray'); ax[1].set_title('MRI'); ax[1].axis('off')\n",
    "        ax[2].imshow(fused_np, cmap='gray'); ax[2].set_title('Fused'); ax[2].axis('off')\n",
    "        plt.tight_layout()\n",
    "        demo_path = os.path.join(RESULTS_DIR, \"demo_inference.png\")\n",
    "        plt.savefig(demo_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"Saved demo:\", demo_path)\n",
    "\n",
    "    # run demo (uncomment to run)\n",
    "    # demo_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e73045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Work/conda/installation/envs/ml/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/libtorch_1745854776362/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized:\n",
      " Dataset pairs: 3208\n",
      " Device: cpu\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/401 [00:03<?, ?it/s, G_Loss=12.9812, D_Loss=1.7494, Recon=1.6208]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 552\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo image pairs found in dataset. Exiting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 552\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     trainer\u001b[38;5;241m.\u001b[39msave_model(NUM_EPOCHS, path\u001b[38;5;241m=\u001b[39mFINAL_MODELS_DIR)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 499\u001b[0m, in \u001b[0;36mDDcGANTrainer.train\u001b[0;34m(self, num_epochs, save_interval)\u001b[0m\n\u001b[1;32m    497\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 499\u001b[0m     g_loss, d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] G: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m D: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m save_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[1], line 423\u001b[0m, in \u001b[0;36mDDcGANTrainer.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    416\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix({\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG_Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD_Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_d_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecon\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_recon_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    420\u001b[0m     })\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 423\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_sample_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mct_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmri_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfused_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# epoch metrics\u001b[39;00m\n\u001b[1;32m    426\u001b[0m avg_g_loss \u001b[38;5;241m=\u001b[39m epoch_g_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader)\n",
      "Cell \u001b[0;32mIn[1], line 445\u001b[0m, in \u001b[0;36mDDcGANTrainer.save_sample_images\u001b[0;34m(self, ct_imgs, mri_imgs, fused_imgs, epoch, batch_idx)\u001b[0m\n\u001b[1;32m    443\u001b[0m ct_np \u001b[38;5;241m=\u001b[39m ((ct_sample \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    444\u001b[0m mri_np \u001b[38;5;241m=\u001b[39m ((mri_sample \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 445\u001b[0m fused_np \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfused_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    448\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(ct_np, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# ddcgan_fusion_improved.py\n",
    "# Improved DDcGAN fusion code (CT + MRI) with balanced losses and feature-level discriminator\n",
    "# Created: 2025-08-31\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import kornia\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Model blocks\n",
    "# ---------------------------\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, filter_size, strides, kernel=3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, filter_size, kernel, strides, kernel // 2),\n",
    "            nn.BatchNorm2d(filter_size),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=2, out_channels=48, constant_feature_map=48):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            Block(in_channels, constant_feature_map, 1),\n",
    "            Block(constant_feature_map, constant_feature_map, 2),\n",
    "            Block(constant_feature_map, constant_feature_map, 1),\n",
    "            Block(constant_feature_map, constant_feature_map, 2),\n",
    "            Block(constant_feature_map, out_channels, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=48, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, 3, 1, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(16, out_channels, 3, 1, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=2, out_channels=1, encoder_constant_features=48):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_channels, encoder_constant_features, encoder_constant_features)\n",
    "        self.decoder = Decoder(encoder_constant_features, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class FeatureDiscriminator(nn.Module):\n",
    "    \"\"\"Discriminator that operates on encoder feature maps (smaller spatial dims).\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=48):\n",
    "        super().__init__()\n",
    "        # small conv net to judge feature realism\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset\n",
    "# ---------------------------\n",
    "class CTMRIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, img_size=(256, 256)):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.image_pairs = []\n",
    "\n",
    "        ct_dir = self.root_dir / \"CT\"\n",
    "        mri_dir = self.root_dir / \"MRI\"\n",
    "\n",
    "        if ct_dir.exists() and mri_dir.exists():\n",
    "            ct_files = sorted(list(ct_dir.glob(\"*.png\")))\n",
    "            mri_files = sorted(list(mri_dir.glob(\"*.png\")))\n",
    "            for ct_file in ct_files:\n",
    "                mri_file = mri_dir / ct_file.name\n",
    "                if mri_file.exists():\n",
    "                    self.image_pairs.append((str(ct_file), str(mri_file)))\n",
    "                else:\n",
    "                    # try other extensions if not found\n",
    "                    for ext in (\".jpg\", \".jpeg\", \".bmp\", \".tif\"):\n",
    "                        alt = mri_dir / (ct_file.stem + ext)\n",
    "                        if alt.exists():\n",
    "                            self.image_pairs.append((str(ct_file), str(alt)))\n",
    "                            break\n",
    "        else:\n",
    "            print(f\"Warning: Dataset directories not found at {root_dir}. expected CT/ and MRI/\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ct_path, mri_path = self.image_pairs[idx]\n",
    "        ct_img = Image.open(ct_path).convert(\"L\")\n",
    "        mri_img = Image.open(mri_path).convert(\"L\")\n",
    "        ct_img = ct_img.resize(self.img_size, Image.Resampling.BILINEAR)\n",
    "        mri_img = mri_img.resize(self.img_size, Image.Resampling.BILINEAR)\n",
    "\n",
    "        if self.transform:\n",
    "            ct_tensor = self.transform(ct_img)\n",
    "            mri_tensor = self.transform(mri_img)\n",
    "        else:\n",
    "            ct_tensor = transforms.ToTensor()(ct_img)\n",
    "            mri_tensor = transforms.ToTensor()(mri_img)\n",
    "            ct_tensor = ct_tensor * 2.0 - 1.0\n",
    "            mri_tensor = mri_tensor * 2.0 - 1.0\n",
    "\n",
    "        return ct_tensor, mri_tensor\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Training utilities\n",
    "# ---------------------------\n",
    "RESULTS_DIR = \"results/ddcgan_fusion_improved\"\n",
    "SAMPLES_DIR = f\"{RESULTS_DIR}/samples\"\n",
    "PLOTS_DIR = f\"{RESULTS_DIR}/plots\"\n",
    "CHECKPOINTS_DIR = \"checkpoints/ddcgan_fusion_improved\"\n",
    "FINAL_MODELS_DIR = \"checkpoints/final/ddcgan_fusion_improved\"\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if \"Conv\" in classname:\n",
    "        if hasattr(m, \"weight\") and m.weight is not None:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif \"BatchNorm\" in classname:\n",
    "        if hasattr(m, \"weight\") and m.weight is not None:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "def gradient_loss(pred, target):\n",
    "    # Sobel gradients via kornia\n",
    "    pred_grad = kornia.filters.sobel(pred)\n",
    "    target_grad = kornia.filters.sobel(target)\n",
    "    return torch.nn.functional.l1_loss(pred_grad, target_grad)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Trainer class\n",
    "# ---------------------------\n",
    "class DDcGANTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path,\n",
    "        batch_size=8,\n",
    "        lr=0.0002,\n",
    "        img_size=(256, 256),\n",
    "        device=\"cuda\",\n",
    "        feature_disc_weight=1.0,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.lr = lr\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "        self.dataset = CTMRIDataset(dataset_path, transform=transform, img_size=img_size)\n",
    "        self.dataloader = DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        # Models\n",
    "        self.generator = Generator(in_channels=2, out_channels=1).to(self.device)\n",
    "        self.discriminator1 = Discriminator(in_channels=1).to(self.device)\n",
    "        self.discriminator2 = Discriminator(in_channels=1).to(self.device)\n",
    "        self.feature_disc = FeatureDiscriminator(in_channels=48).to(self.device)\n",
    "\n",
    "        # Initialize\n",
    "        self.generator.apply(weights_init)\n",
    "        self.discriminator1.apply(weights_init)\n",
    "        self.discriminator2.apply(weights_init)\n",
    "        self.feature_disc.apply(weights_init)\n",
    "\n",
    "        # Optimizers\n",
    "        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        self.d1_optimizer = optim.Adam(self.discriminator1.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        self.d2_optimizer = optim.Adam(self.discriminator2.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        self.fd_optimizer = optim.Adam(self.feature_disc.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "        # Losses\n",
    "        self.adversarial_loss = nn.MSELoss()\n",
    "        self.reconstruction_loss = nn.L1Loss()\n",
    "        self.ssim_loss_fn = kornia.losses.SSIMLoss(window_size=11, reduction=\"mean\")\n",
    "\n",
    "        # Weights (tune these)\n",
    "        self.w_recon = 5.0\n",
    "        self.w_grad = 5.0\n",
    "        self.w_ssim = 2.0\n",
    "        self.w_feature_adv = feature_disc_weight\n",
    "\n",
    "        # tracking\n",
    "        self.g_losses = []\n",
    "        self.d_losses = []\n",
    "        self.recon_losses = []\n",
    "\n",
    "        print(\"Trainer initialized:\")\n",
    "        print(f\" Dataset pairs: {len(self.dataset)}\")\n",
    "        print(f\" Device: {self.device}\")\n",
    "\n",
    "    def ssim_loss(self, img1, img2):\n",
    "        return self.ssim_loss_fn(img1, img2)\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.generator.train()\n",
    "        self.discriminator1.train()\n",
    "        self.discriminator2.train()\n",
    "        self.feature_disc.train()\n",
    "\n",
    "        epoch_g_loss = 0\n",
    "        epoch_d_loss = 0\n",
    "        epoch_recon_loss = 0\n",
    "\n",
    "        pbar = tqdm(self.dataloader, desc=f\"Epoch {epoch}\")\n",
    "\n",
    "        for i, (ct_imgs, mri_imgs) in enumerate(pbar):\n",
    "            ct_imgs = ct_imgs.to(self.device)\n",
    "            mri_imgs = mri_imgs.to(self.device)\n",
    "            batch_size = ct_imgs.size(0)\n",
    "\n",
    "            # Input for generator: concatenate CT and MRI\n",
    "            input_imgs = torch.cat([ct_imgs, mri_imgs], dim=1)\n",
    "\n",
    "            real_label = torch.ones(batch_size, 1).to(self.device)\n",
    "            fake_label = torch.zeros(batch_size, 1).to(self.device)\n",
    "\n",
    "            # ---------------------\n",
    "            # Train Discriminators on image space\n",
    "            # ---------------------\n",
    "            with torch.no_grad():\n",
    "                fake_fused, fake_feat = self.generator(input_imgs)\n",
    "\n",
    "            # Choose real samples for image discriminators randomly from CT/MRI\n",
    "            # This prevents the discriminator from always using CT as \"real fused\"\n",
    "            if random.random() < 0.5:\n",
    "                real_fused = ct_imgs\n",
    "            else:\n",
    "                real_fused = mri_imgs\n",
    "\n",
    "            # D1 (global)\n",
    "            self.d1_optimizer.zero_grad()\n",
    "            real_pred1 = self.discriminator1(real_fused)\n",
    "            fake_pred1 = self.discriminator1(fake_fused.detach())\n",
    "            d1_loss = (self.adversarial_loss(real_pred1, real_label) + self.adversarial_loss(fake_pred1, fake_label)) / 2\n",
    "            d1_loss.backward()\n",
    "            self.d1_optimizer.step()\n",
    "\n",
    "            # D2 (local)\n",
    "            self.d2_optimizer.zero_grad()\n",
    "            real_pred2 = self.discriminator2(real_fused)\n",
    "            fake_pred2 = self.discriminator2(fake_fused.detach())\n",
    "            d2_loss = (self.adversarial_loss(real_pred2, real_label) + self.adversarial_loss(fake_pred2, fake_label)) / 2\n",
    "            d2_loss.backward()\n",
    "            self.d2_optimizer.step()\n",
    "\n",
    "            # Feature discriminator\n",
    "            self.fd_optimizer.zero_grad()\n",
    "            # For feature discriminator we use encoder outputs of \"real\" inputs\n",
    "            # Build real features by encoding a mixed real input (either CT or MRI or concatenated)\n",
    "            with torch.no_grad():\n",
    "                # encode CT and MRI separately and also encode concatenated as real examples\n",
    "                ct_encoded = self.generator.encoder(torch.cat([ct_imgs, ct_imgs], dim=1))\n",
    "                mri_encoded = self.generator.encoder(torch.cat([mri_imgs, mri_imgs], dim=1))\n",
    "\n",
    "            # Randomly choose real feature source\n",
    "            if random.random() < 0.5:\n",
    "                real_feat = ct_encoded\n",
    "            else:\n",
    "                real_feat = mri_encoded\n",
    "\n",
    "            fake_feat_detach = fake_feat.detach()\n",
    "            real_feat_pred = self.feature_disc(real_feat)\n",
    "            fake_feat_pred = self.feature_disc(fake_feat_detach)\n",
    "            fd_loss = (self.adversarial_loss(real_feat_pred, real_label) + self.adversarial_loss(fake_feat_pred, fake_label)) / 2\n",
    "            fd_loss.backward()\n",
    "            self.fd_optimizer.step()\n",
    "\n",
    "            total_d_loss = d1_loss + d2_loss + fd_loss\n",
    "\n",
    "            # ---------------------\n",
    "            # Train Generator\n",
    "            # ---------------------\n",
    "            self.g_optimizer.zero_grad()\n",
    "            fused_imgs, feat = self.generator(input_imgs)\n",
    "\n",
    "            # Adversarial (image space)\n",
    "            pred1 = self.discriminator1(fused_imgs)\n",
    "            pred2 = self.discriminator2(fused_imgs)\n",
    "            g_adv_loss = (self.adversarial_loss(pred1, real_label) + self.adversarial_loss(pred2, real_label)) / 2\n",
    "\n",
    "            # Feature adversarial: fool feature discriminator\n",
    "            feat_pred = self.feature_disc(feat)\n",
    "            g_feat_adv_loss = self.adversarial_loss(feat_pred, real_label)\n",
    "\n",
    "            # Reconstruction: with both modalities (L1)\n",
    "            g_recon_loss = self.reconstruction_loss(fused_imgs, ct_imgs) + self.reconstruction_loss(fused_imgs, mri_imgs)\n",
    "\n",
    "            # Gradient loss for both\n",
    "            g_grad_loss = gradient_loss(fused_imgs, ct_imgs) + gradient_loss(fused_imgs, mri_imgs)\n",
    "\n",
    "            # SSIM loss (both)\n",
    "            g_ssim_loss = self.ssim_loss(fused_imgs, ct_imgs) + self.ssim_loss(fused_imgs, mri_imgs)\n",
    "\n",
    "            # Total loss (balanced)\n",
    "            g_loss = g_adv_loss + \\\n",
    "                     self.w_recon * g_recon_loss + \\\n",
    "                     self.w_grad * g_grad_loss + \\\n",
    "                     self.w_ssim * g_ssim_loss + \\\n",
    "                     self.w_feature_adv * g_feat_adv_loss\n",
    "\n",
    "            g_loss.backward()\n",
    "            self.g_optimizer.step()\n",
    "\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += total_d_loss.item()\n",
    "            epoch_recon_loss += g_recon_loss.item()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"G_Loss\": f\"{g_loss.item():.4f}\",\n",
    "                \"D_Loss\": f\"{total_d_loss.item():.4f}\",\n",
    "                \"Recon\": f\"{g_recon_loss.item():.4f}\",\n",
    "            })\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                self.save_sample_images(ct_imgs, mri_imgs, fused_imgs, epoch, i)\n",
    "\n",
    "        # epoch metrics\n",
    "        avg_g_loss = epoch_g_loss / len(self.dataloader)\n",
    "        avg_d_loss = epoch_d_loss / len(self.dataloader)\n",
    "        avg_recon_loss = epoch_recon_loss / len(self.dataloader)\n",
    "\n",
    "        self.g_losses.append(avg_g_loss)\n",
    "        self.d_losses.append(avg_d_loss)\n",
    "        self.recon_losses.append(avg_recon_loss)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch} summary: G {avg_g_loss:.4f}, D {avg_d_loss:.4f}, Recon {avg_recon_loss:.4f}\")\n",
    "\n",
    "        return avg_g_loss, avg_d_loss\n",
    "\n",
    "    def save_sample_images(self, ct_imgs, mri_imgs, fused_imgs, epoch, batch_idx):\n",
    "        os.makedirs(SAMPLES_DIR, exist_ok=True)\n",
    "        ct_sample = ct_imgs[0].cpu()\n",
    "        mri_sample = mri_imgs[0].cpu()\n",
    "        fused_sample = fused_imgs[0].cpu()\n",
    "        ct_np = ((ct_sample + 1) / 2).squeeze().numpy()\n",
    "        mri_np = ((mri_sample + 1) / 2).squeeze().numpy()\n",
    "        fused_np = ((fused_sample + 1) / 2).squeeze().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "        axes[0].imshow(ct_np, cmap=\"gray\")\n",
    "        axes[0].set_title(\"CT\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(mri_np, cmap=\"gray\")\n",
    "        axes[1].set_title(\"MRI\")\n",
    "        axes[1].axis(\"off\")\n",
    "        axes[2].imshow(fused_np, cmap=\"gray\")\n",
    "        axes[2].set_title(\"Fused\")\n",
    "        axes[2].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{SAMPLES_DIR}/epoch_{epoch}_batch_{batch_idx}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self, epoch, path=CHECKPOINTS_DIR):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"generator_state_dict\": self.generator.state_dict(),\n",
    "            \"discriminator1_state_dict\": self.discriminator1.state_dict(),\n",
    "            \"discriminator2_state_dict\": self.discriminator2.state_dict(),\n",
    "            \"feature_disc_state_dict\": self.feature_disc.state_dict(),\n",
    "            \"g_optimizer_state_dict\": self.g_optimizer.state_dict(),\n",
    "            \"d1_optimizer_state_dict\": self.d1_optimizer.state_dict(),\n",
    "            \"d2_optimizer_state_dict\": self.d2_optimizer.state_dict(),\n",
    "            \"fd_optimizer_state_dict\": self.fd_optimizer.state_dict(),\n",
    "            \"g_losses\": self.g_losses,\n",
    "            \"d_losses\": self.d_losses,\n",
    "            \"recon_losses\": self.recon_losses,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"{path}/ddcgan_improved_epoch_{epoch}.pth\")\n",
    "        print(f\"Saved checkpoint epoch {epoch} -> {path}\")\n",
    "\n",
    "    def load_model(self, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.generator.load_state_dict(checkpoint[\"generator_state_dict\"])\\\n",
    "            if \"generator_state_dict\" in checkpoint else None\n",
    "        self.discriminator1.load_state_dict(checkpoint[\"discriminator1_state_dict\"])\\\n",
    "            if \"discriminator1_state_dict\" in checkpoint else None\n",
    "        self.discriminator2.load_state_dict(checkpoint[\"discriminator2_state_dict\"])\\\n",
    "            if \"discriminator2_state_dict\" in checkpoint else None\n",
    "        self.feature_disc.load_state_dict(checkpoint[\"feature_disc_state_dict\"])\\\n",
    "            if \"feature_disc_state_dict\" in checkpoint else None\n",
    "        self.g_losses = checkpoint.get(\"g_losses\", [])\n",
    "        self.d_losses = checkpoint.get(\"d_losses\", [])\n",
    "        self.recon_losses = checkpoint.get(\"recon_losses\", [])\n",
    "        return checkpoint.get(\"epoch\", 0)\n",
    "\n",
    "    def train(self, num_epochs=50, save_interval=5):\n",
    "        print(\"Starting training...\")\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            g_loss, d_loss = self.train_epoch(epoch)\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] G: {g_loss:.4f} D: {d_loss:.4f}\")\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_model(epoch)\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Training finished in {total_time:.2f}s\")\n",
    "        self.plot_training_metrics()\n",
    "\n",
    "    def plot_training_metrics(self):\n",
    "        os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "        epochs = range(1, len(self.g_losses) + 1)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(epochs, self.g_losses, label=\"Generator Loss\")\n",
    "        plt.plot(epochs, self.d_losses, label=\"Discriminator Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")\n",
    "        plt.savefig(f\"{PLOTS_DIR}/losses.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Demo / Main\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # basic config - change these paths to suit your setup\n",
    "    DATASET_PATH = \"../Dataset/train\"  # expects CT/ and MRI/ subfolders\n",
    "    BATCH_SIZE = 8\n",
    "    LR = 2e-4\n",
    "    NUM_EPOCHS = 30\n",
    "    IMG_SIZE = (256, 256)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    os.makedirs(SAMPLES_DIR, exist_ok=True)\n",
    "    os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
    "    os.makedirs(FINAL_MODELS_DIR, exist_ok=True)\n",
    "\n",
    "    trainer = DDcGANTrainer(\n",
    "        dataset_path=DATASET_PATH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr=LR,\n",
    "        img_size=IMG_SIZE,\n",
    "        device=device,\n",
    "        feature_disc_weight=1.0,\n",
    "    )\n",
    "\n",
    "    # quick sanity check: minimal dataset\n",
    "    if len(trainer.dataset) == 0:\n",
    "        print(\"No image pairs found in dataset. Exiting.\")\n",
    "    else:\n",
    "        trainer.train(num_epochs=NUM_EPOCHS, save_interval=5)\n",
    "        trainer.save_model(NUM_EPOCHS, path=FINAL_MODELS_DIR)\n",
    "\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268756b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
