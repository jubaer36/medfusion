{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 1: Starting imports and device configuration\n",
      "Cell 1: Imports completed successfully\n",
      "Using device: cuda\n",
      "CUDA device name: NVIDIA GeForce RTX 3060\n",
      "CUDA version: 12.4\n",
      "Cell 1: Device configuration completed\n",
      "Cell 1: Imports completed successfully\n",
      "Using device: cuda\n",
      "CUDA device name: NVIDIA GeForce RTX 3060\n",
      "CUDA version: 12.4\n",
      "Cell 1: Device configuration completed\n"
     ]
    }
   ],
   "source": [
    "print('Cell 1: Starting imports and device configuration')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torchvision.models import VGG19_Weights\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('Cell 1: Imports completed successfully')\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA device name: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA version: {torch.version.cuda}')\n",
    "print('Cell 1: Device configuration completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 2: Starting configuration setup\n",
      "Configuration set:\n",
      "  batch_size: 8\n",
      "  lr: 0.0001\n",
      "  num_epochs: 2\n",
      "  device: cuda\n",
      "  save_path: fusion_model_improved.pth\n",
      "  w_ssim: 1.0\n",
      "  w_grad: 1.0\n",
      "  w_perc: 0.1\n",
      "  w_int: 5.0\n",
      "  image_size: (256, 256)\n",
      "Cell 2: Configuration setup completed\n"
     ]
    }
   ],
   "source": [
    "print('Cell 2: Starting configuration setup')\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'batch_size': 8,\n",
    "    'lr': 1e-4,\n",
    "    'num_epochs': 2,  # INCREASED: Train for more epochs for better convergence\n",
    "    'device': device,\n",
    "    'save_path': 'fusion_model_improved.pth',\n",
    "    'w_ssim': 1.0,\n",
    "    'w_grad': 1.0,\n",
    "    'w_perc': 0.1,\n",
    "    'w_int': 5.0,     # ADDED: Weight for the new L1 intensity loss\n",
    "    'image_size': (256, 256)\n",
    "}\n",
    "\n",
    "print('Configuration set:')\n",
    "for key, value in config.items():\n",
    "    print(f'  {key}: {value}')\n",
    "print('Cell 2: Configuration setup completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 3: Starting LLVIPDataset class definition with Augmentation\n",
      "Cell 3: LLVIPDataset class definition completed\n"
     ]
    }
   ],
   "source": [
    "print('Cell 3: Starting LLVIPDataset class definition with Augmentation')\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "# Dataset class for LLVIP dataset\n",
    "class LLVIPDataset(Dataset):\n",
    "    \"\"\"Dataset for LLVIP visible (RGB) and infrared (grayscale) images.\"\"\"\n",
    "    def __init__(self, root_dir, train=True): # Removed transform argument\n",
    "        self.train = train\n",
    "        mode = 'train' if train else 'test'\n",
    "        self.vis_path = os.path.join(root_dir, 'visible', mode)\n",
    "        self.ir_path = os.path.join(root_dir, 'infrared', mode)\n",
    "        \n",
    "        self.vis_images = sorted([f for f in os.listdir(self.vis_path) \n",
    "                                 if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif'))])\n",
    "        self.ir_images = sorted([f for f in os.listdir(self.ir_path) \n",
    "                                 if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif'))])\n",
    "        \n",
    "        self.pairs = []\n",
    "        for vis_img in self.vis_images:\n",
    "            if vis_img in self.ir_images:\n",
    "                self.pairs.append(vis_img)\n",
    "        \n",
    "        print(f\"Found {len(self.pairs)} image pairs in {mode} set\")\n",
    "\n",
    "        # Define transforms internally\n",
    "        self.transform_vis = transforms.Compose([\n",
    "            transforms.Resize(config['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.transform_ir = transforms.Compose([\n",
    "            transforms.Resize(config['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.pairs[idx]\n",
    "        \n",
    "        vis_img = Image.open(os.path.join(self.vis_path, img_name)).convert('RGB')\n",
    "        ir_img = Image.open(os.path.join(self.ir_path, img_name)).convert('L')\n",
    "        \n",
    "        # Apply initial transforms\n",
    "        vis_tensor = self.transform_vis(vis_img)\n",
    "        ir_tensor = self.transform_ir(ir_img)\n",
    "\n",
    "        # --- ADDED: Paired Data Augmentation for training set ---\n",
    "        if self.train:\n",
    "            # Random horizontal flip\n",
    "            if random.random() > 0.5:\n",
    "                vis_tensor = TF.hflip(vis_tensor)\n",
    "                ir_tensor = TF.hflip(ir_tensor)\n",
    "\n",
    "            # Random rotation\n",
    "            if random.random() > 0.5:\n",
    "                angle = random.uniform(-10, 10)\n",
    "                vis_tensor = TF.rotate(vis_tensor, angle)\n",
    "                ir_tensor = TF.rotate(ir_tensor, angle)\n",
    "            \n",
    "        return vis_tensor, ir_tensor, img_name\n",
    "\n",
    "print('Cell 3: LLVIPDataset class definition completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 4: Starting simplified collate function setup\n",
      "Collate function simplified for batching tensors\n",
      "Cell 4: Simplified collate function setup completed\n"
     ]
    }
   ],
   "source": [
    "print('Cell 4: Starting simplified collate function setup')\n",
    "\n",
    "# The transforms are now inside the Dataset class. This function just batches the tensors.\n",
    "def collate_fn(batch):\n",
    "    vis_tensors, ir_tensors, names = zip(*batch)\n",
    "    \n",
    "    vis_batch = torch.stack(vis_tensors)\n",
    "    ir_batch = torch.stack(ir_tensors)\n",
    "    \n",
    "    return vis_batch, ir_batch, names\n",
    "\n",
    "print('Collate function simplified for batching tensors')\n",
    "print('Cell 4: Simplified collate function setup completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 5: This cell is currently empty - placeholder for future code\n"
     ]
    }
   ],
   "source": [
    "print('Cell 5: This cell is currently empty - placeholder for future code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 6: Starting loss components definition\n",
      "Cell 6: Loss components definition completed\n"
     ]
    }
   ],
   "source": [
    "print('Cell 6: Starting loss components definition')\n",
    "# Loss components (from the original codebase)\n",
    "class SSIM(nn.Module):\n",
    "    def __init__(self, window_size=11, C1=0.01**2, C2=0.03**2):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        gauss = cv2.getGaussianKernel(window_size, window_size/6)\n",
    "        gauss = gauss @ gauss.T\n",
    "        w = torch.from_numpy(gauss.astype(np.float32))[None, None]\n",
    "        self.register_buffer('window', w)\n",
    "        print(f'SSIM initialized with window_size={window_size}')\n",
    "\n",
    "    def _filt(self, x):\n",
    "        pad = self.window_size//2\n",
    "        # CORRECTED: Cast window to match input tensor's dtype\n",
    "        window = self.window.to(device=x.device, dtype=x.dtype)\n",
    "        return F.conv2d(x, window, padding=pad, groups=x.size(1))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x,y: (B,1,H,W) in [0,1]\n",
    "        mu_x = self._filt(x)\n",
    "        mu_y = self._filt(y)\n",
    "        mu_x2, mu_y2, mu_xy = mu_x*mu_x, mu_y*mu_y, mu_x*mu_y\n",
    "        sigma_x2 = self._filt(x*x) - mu_x2\n",
    "        sigma_y2 = self._filt(y*y) - mu_y2\n",
    "        sigma_xy = self._filt(x*y) - mu_xy\n",
    "        ssim = ((2*mu_xy + self.C1)*(2*sigma_xy + self.C2)) / ((mu_x2 + mu_y2 + self.C1)*(sigma_x2 + sigma_y2 + self.C2) + 1e-8)\n",
    "        return ssim.mean()\n",
    "\n",
    "class GradientLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        kx = np.array([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=np.float32)\n",
    "        ky = np.array([[1,2,1],[0,0,0],[-1,-2,-1]], dtype=np.float32)\n",
    "        self.register_buffer('kx', torch.from_numpy(kx)[None, None])\n",
    "        self.register_buffer('ky', torch.from_numpy(ky)[None, None])\n",
    "        print('GradientLoss initialized')\n",
    "\n",
    "    def forward(self, fused, vis, ir):\n",
    "        def grad(img):\n",
    "            # Handle both single-channel and multi-channel images\n",
    "            if img.size(1) > 1:\n",
    "                img_gray = img.mean(dim=1, keepdim=True)\n",
    "            else:\n",
    "                img_gray = img\n",
    "            \n",
    "            # CORRECTED: Cast kernels to match input tensor's dtype\n",
    "            kx_ = self.kx.to(device=img_gray.device, dtype=img_gray.dtype)\n",
    "            ky_ = self.ky.to(device=img_gray.device, dtype=img_gray.dtype)\n",
    "\n",
    "            gx = F.conv2d(img_gray, kx_, padding=1)\n",
    "            gy = F.conv2d(img_gray, ky_, padding=1)\n",
    "            return torch.sqrt(gx*gx + gy*gy + 1e-8)\n",
    "            \n",
    "        gF = grad(fused)\n",
    "        gV = grad(vis)\n",
    "        gI = grad(ir)\n",
    "        gT = torch.max(gV, gI)\n",
    "        return F.l1_loss(gF, gT)\n",
    "\n",
    "class VGGPerceptual(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features\n",
    "        self.slice1 = nn.Sequential(*[vgg[i] for i in range(4)])   # relu1_2\n",
    "        self.slice2 = nn.Sequential(*[vgg[i] for i in range(4,9)]) # relu2_2\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.to(device)\n",
    "        print(f'VGGPerceptual initialized on device: {device}')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is 3-channel for VGG\n",
    "        if x.size(1) == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        elif x.size(1) > 3:\n",
    "            x = x[:, :3, :, :]  # Take first 3 channels if more\n",
    "            \n",
    "        mean = torch.tensor([0.485, 0.456, 0.406], device=x.device)[None,:,None,None]\n",
    "        std  = torch.tensor([0.229, 0.224, 0.225], device=x.device)[None,:,None,None]\n",
    "        x = (x - mean) / std\n",
    "        f1 = self.slice1(x)\n",
    "        f2 = self.slice2(f1)\n",
    "        return f1, f2\n",
    "\n",
    "def perceptual_loss(vgg: VGGPerceptual, fused, vis, ir):\n",
    "    fF1, fF2 = vgg(fused)\n",
    "    vF1, vF2 = vgg(vis)\n",
    "    iF1, iF2 = vgg(ir)\n",
    "    return 0.5*(F.l1_loss(fF1, vF1) + F.l1_loss(fF1, iF1) + F.l1_loss(fF2, vF2) + F.l1_loss(fF2, iF2))\n",
    "\n",
    "class FusionLoss(nn.Module):\n",
    "    def __init__(self, device, w_ssim=1.0, w_grad=1.0, w_perc=0.1, w_int=5.0): # Added w_int\n",
    "        super().__init__()\n",
    "        self.ssim = SSIM()\n",
    "        self.grad = GradientLoss()\n",
    "        self.vgg = VGGPerceptual(device)\n",
    "        self.w_ssim = w_ssim\n",
    "        self.w_grad = w_grad\n",
    "        self.w_perc = w_perc\n",
    "        self.w_int = w_int  # Store new weight\n",
    "        print(f'FusionLoss initialized with weights: SSIM={w_ssim}, Grad={w_grad}, Perc={w_perc}, Int={w_int}')\n",
    "        \n",
    "    def forward(self, fused, vis, ir):\n",
    "        # --- ADDED: L1 Intensity Loss ---\n",
    "        # Encourages the fused image to retain the brightest pixels from either input\n",
    "        intensity_target = torch.max(vis, ir)\n",
    "        l_intensity = F.l1_loss(fused, intensity_target)\n",
    "\n",
    "        # --- Grayscale conversions for other losses ---\n",
    "        if fused.size(1) > 1:\n",
    "            fused_gray = fused.mean(dim=1, keepdim=True)\n",
    "        else:\n",
    "            fused_gray = fused\n",
    "        if vis.size(1) > 1:\n",
    "            vis_gray = vis.mean(dim=1, keepdim=True)\n",
    "        else:\n",
    "            vis_gray = vis\n",
    "        if ir.size(1) > 1:\n",
    "            ir_gray = ir.mean(dim=1, keepdim=True)\n",
    "        else:\n",
    "            ir_gray = ir\n",
    "            \n",
    "        # --- Original loss calculations ---\n",
    "        l_ssim = 0.5*(1.0 - self.ssim(fused_gray, vis_gray)) + 0.5*(1.0 - self.ssim(fused_gray, ir_gray))\n",
    "        l_grad = self.grad(fused, vis, ir)\n",
    "        l_perc = perceptual_loss(self.vgg, fused, vis, ir)\n",
    "        \n",
    "        # --- MODIFIED: Combined total loss ---\n",
    "        total = self.w_int * l_intensity + self.w_ssim*l_ssim + self.w_grad*l_grad + self.w_perc*l_perc\n",
    "        \n",
    "        # --- MODIFIED: Return dictionary with new loss component ---\n",
    "        return total, {\"intensity\": l_intensity.item(), \"ssim\": l_ssim.item(), \"grad\": l_grad.item(), \"perc\": l_perc.item()}\n",
    "\n",
    "print('Cell 6: Loss components definition completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 7: Starting UNetFusion model definition\n",
      "Cell 7: UNetFusion model definition completed\n"
     ]
    }
   ],
   "source": [
    "print('Cell 7: Starting UNetFusion model definition')\n",
    "# Simplified U-Net based Fusion Model\n",
    "class UNetFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print('Initializing UNetFusion model...')\n",
    "        \n",
    "        # Encoder (pretrained ResNet backbone)\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1\n",
    "        )\n",
    "        self.encoder2 = resnet.layer2\n",
    "        self.encoder3 = resnet.layer3\n",
    "        self.encoder4 = resnet.layer4\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.Conv2d(2048, 1024, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Conv2d(2048, 512, 3, padding=1),  # 1024 from decoder + 1024 from encoder3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 256, 3, padding=1),  # 512 from decoder + 512 from encoder2\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        )\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, 3, padding=1),  # 256 from decoder + 256 from encoder1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        )\n",
    "        \n",
    "        # Final convolution - CORRECTED\n",
    "        self.final_conv = nn.Sequential(\n",
    "            # Add the missing Upsample layer to go from 128x128 to 256x256\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=1), # Input channels from decoder1 is 128\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        print('UNetFusion model initialized successfully')\n",
    "\n",
    "    def forward(self, vis, ir):\n",
    "        # Process both inputs through the encoder\n",
    "        vis1 = self.encoder1(vis)\n",
    "        vis2 = self.encoder2(vis1)\n",
    "        vis3 = self.encoder3(vis2)\n",
    "        vis4 = self.encoder4(vis3)\n",
    "        \n",
    "        ir1 = self.encoder1(ir)\n",
    "        ir2 = self.encoder2(ir1)\n",
    "        ir3 = self.encoder3(ir2)\n",
    "        ir4 = self.encoder4(ir3)\n",
    "        \n",
    "        # Fuse features at each level (simple addition)\n",
    "        fused4 = vis4 + ir4\n",
    "        fused3 = vis3 + ir3\n",
    "        fused2 = vis2 + ir2\n",
    "        fused1 = vis1 + ir1\n",
    "        \n",
    "        # Decode with skip connections\n",
    "        d4 = self.decoder4(fused4)\n",
    "        d3 = self.decoder3(torch.cat([d4, fused3], dim=1))\n",
    "        d2 = self.decoder2(torch.cat([d3, fused2], dim=1))\n",
    "        d1 = self.decoder1(torch.cat([d2, fused1], dim=1))\n",
    "        \n",
    "        # Final output\n",
    "        return self.final_conv(d1)\n",
    "\n",
    "print('Cell 7: UNetFusion model definition completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 8: Starting train_model function definition\n",
      "Cell 8: train_model function definition completed\n"
     ]
    }
   ],
   "source": [
    "print('Cell 8: Starting train_model function definition')\n",
    "def train_model():\n",
    "    print('Initializing training components...')\n",
    "    import time\n",
    "    # Initialize model, dataset, and loss\n",
    "    model = UNetFusion().to(config['device'])\n",
    "    full_dataset = LLVIPDataset('LLVIP', train=True)\n",
    "\n",
    "    # Split dataset into train and validation (80% train, 20% val)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True,\n",
    "                                 num_workers=4, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False,\n",
    "                               num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    scaler = GradScaler(device='cuda')\n",
    "    criterion = FusionLoss(config['device'])\n",
    "\n",
    "    print(f'Full dataset size: {len(full_dataset)} samples')\n",
    "    print(f'Training set size: {len(train_dataset)} samples')\n",
    "    print(f'Validation set size: {len(val_dataset)} samples')\n",
    "    print(f'Batch size: {config[\"batch_size\"]}')\n",
    "    print(f'Number of training batches per epoch: {len(train_dataloader)}')\n",
    "    print(f'Number of validation batches per epoch: {len(val_dataloader)}')\n",
    "    print(f'Total epochs: {config[\"num_epochs\"]}')\n",
    "    print('Starting training loop...')\n",
    "\n",
    "    # Track best validation loss for model saving\n",
    "    best_val_loss = float('inf')\n",
    "    training_history = []\n",
    "\n",
    "    # Training loop with real-time updates\n",
    "    epoch_bar = tqdm(range(config['num_epochs']), desc='ðŸŽ¯ Training Progress', unit='epoch',\n",
    "                     bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',\n",
    "                     ncols=100, miniters=1, mininterval=0.1)\n",
    "\n",
    "    for epoch in epoch_bar:\n",
    "        epoch_start_time = time.time()\n",
    "        tqdm.write(f'\\nðŸš€ Epoch {epoch+1}/{config[\"num_epochs\"]} - Starting...')\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        ssim_loss = 0\n",
    "        grad_loss = 0\n",
    "        perc_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # Training phase with enhanced progress bar\n",
    "        train_bar = tqdm(train_dataloader,\n",
    "                        desc=f'ðŸ“š Training Epoch {epoch+1}',\n",
    "                        leave=False,\n",
    "                        unit='batch',\n",
    "                        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}',\n",
    "                        ncols=120, miniters=1, mininterval=0.05)\n",
    "\n",
    "        train_start_time = time.time()\n",
    "        for batch_idx, (vis, ir, _) in enumerate(train_bar):\n",
    "            batch_start_time = time.time()\n",
    "            vis = vis.to(config['device'])\n",
    "            ir = ir.to(config['device'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                fused = model(vis, ir)\n",
    "                loss, loss_components = criterion(fused, vis, ir)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            total_loss += loss.item()\n",
    "            ssim_loss += loss_components['ssim']\n",
    "            grad_loss += loss_components['grad']\n",
    "            perc_loss += loss_components['perc']\n",
    "            batch_count += 1\n",
    "\n",
    "            # Update progress bar with detailed metrics - force refresh\n",
    "            current_loss = total_loss / batch_count\n",
    "            current_ssim = ssim_loss / batch_count\n",
    "            current_grad = grad_loss / batch_count\n",
    "            current_perc = perc_loss / batch_count\n",
    "\n",
    "            train_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'avg_loss': f'{current_loss:.4f}',\n",
    "                'ssim': f'{current_ssim:.4f}',\n",
    "                'grad': f'{current_grad:.4f}',\n",
    "                'perc': f'{current_perc:.4f}',\n",
    "                'batch_time': f'{batch_time:.3f}s'\n",
    "            }, refresh=True)\n",
    "\n",
    "            # Force refresh every few batches\n",
    "            if batch_idx % 5 == 0:\n",
    "                train_bar.refresh()\n",
    "\n",
    "        train_time = time.time() - train_start_time\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        avg_train_ssim = ssim_loss / len(train_dataloader)\n",
    "        avg_train_grad = grad_loss / len(train_dataloader)\n",
    "        avg_train_perc = perc_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation phase with enhanced progress bar\n",
    "        model.eval()\n",
    "        val_total_loss = 0\n",
    "        val_ssim_loss = 0\n",
    "        val_grad_loss = 0\n",
    "        val_perc_loss = 0\n",
    "        val_batch_count = 0\n",
    "\n",
    "        val_bar = tqdm(val_dataloader,\n",
    "                      desc=f'âœ… Validating Epoch {epoch+1}',\n",
    "                      leave=False,\n",
    "                      unit='batch',\n",
    "                      bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}',\n",
    "                      ncols=120, miniters=1, mininterval=0.05)\n",
    "\n",
    "        val_start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (vis, ir, _) in enumerate(val_bar):\n",
    "                batch_start_time = time.time()\n",
    "                vis = vis.to(config['device'])\n",
    "                ir = ir.to(config['device'])\n",
    "\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fused = model(vis, ir)\n",
    "                    loss, loss_components = criterion(fused, vis, ir)\n",
    "\n",
    "                batch_time = time.time() - batch_start_time\n",
    "                val_total_loss += loss.item()\n",
    "                val_ssim_loss += loss_components['ssim']\n",
    "                val_grad_loss += loss_components['grad']\n",
    "                val_perc_loss += loss_components['perc']\n",
    "                val_batch_count += 1\n",
    "\n",
    "                # Update validation progress bar - force refresh\n",
    "                current_val_loss = val_total_loss / val_batch_count\n",
    "                val_bar.set_postfix({\n",
    "                    'val_loss': f'{loss.item():.4f}',\n",
    "                    'avg_val_loss': f'{current_val_loss:.4f}',\n",
    "                    'batch_time': f'{batch_time:.3f}s'\n",
    "                }, refresh=True)\n",
    "\n",
    "                # Force refresh every few batches\n",
    "                if batch_idx % 3 == 0:\n",
    "                    val_bar.refresh()\n",
    "\n",
    "        val_time = time.time() - val_start_time\n",
    "        avg_val_loss = val_total_loss / len(val_dataloader)\n",
    "        avg_val_ssim = val_ssim_loss / len(val_dataloader)\n",
    "        avg_val_grad = val_grad_loss / len(val_dataloader)\n",
    "        avg_val_perc = val_perc_loss / len(val_dataloader)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        # Store epoch metrics\n",
    "        epoch_metrics = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'train_ssim': avg_train_ssim,\n",
    "            'train_grad': avg_train_grad,\n",
    "            'train_perc': avg_train_perc,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_ssim': avg_val_ssim,\n",
    "            'val_grad': avg_val_grad,\n",
    "            'val_perc': avg_val_perc,\n",
    "            'epoch_time': epoch_time,\n",
    "            'train_time': train_time,\n",
    "            'val_time': val_time\n",
    "        }\n",
    "        training_history.append(epoch_metrics)\n",
    "\n",
    "        # Update epoch progress bar with final metrics\n",
    "        epoch_bar.set_postfix({\n",
    "            'train_loss': f'{avg_train_loss:.4f}',\n",
    "            'val_loss': f'{avg_val_loss:.4f}',\n",
    "            'time': f'{epoch_time:.1f}s'\n",
    "        }, refresh=True)\n",
    "        epoch_bar.refresh()\n",
    "\n",
    "        # Print comprehensive epoch summary\n",
    "        tqdm.write(f'ðŸ“Š Epoch {epoch+1}/{config[\"num_epochs\"]} Summary (â±ï¸ {epoch_time:.1f}s total):')\n",
    "        tqdm.write(f'  ðŸŽ“ Training (â±ï¸ {train_time:.1f}s):')\n",
    "        tqdm.write(f'    Loss: {avg_train_loss:.4f} | SSIM: {avg_train_ssim:.4f} | Grad: {avg_train_grad:.4f} | Perc: {avg_train_perc:.4f}')\n",
    "        tqdm.write(f'  âœ… Validation (â±ï¸ {val_time:.1f}s):')\n",
    "        tqdm.write(f'    Loss: {avg_val_loss:.4f} | SSIM: {avg_val_ssim:.4f} | Grad: {avg_val_grad:.4f} | Perc: {avg_val_perc:.4f}')\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'config': config,\n",
    "                'epoch': epoch + 1,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'training_history': training_history\n",
    "            }, config['save_path'])\n",
    "            tqdm.write(f'ðŸ’¾ Best model saved! (Val Loss: {best_val_loss:.4f})')\n",
    "\n",
    "    # Final summary\n",
    "    tqdm.write(f'\\nðŸŽ‰ Training completed!')\n",
    "    tqdm.write(f'ðŸ† Best validation loss: {best_val_loss:.4f}')\n",
    "    tqdm.write(f'ðŸ’¾ Final model saved to {config[\"save_path\"]}')\n",
    "\n",
    "    # Print training history summary\n",
    "    tqdm.write(f'\\nðŸ“ˆ Training History Summary:')\n",
    "    tqdm.write(f'{\"Epoch\":<5} {\"Train Loss\":<12} {\"Val Loss\":<12} {\"Train SSIM\":<12} {\"Val SSIM\":<12} {\"Time\":<8}')\n",
    "    tqdm.write('-' * 80)\n",
    "    for metrics in training_history[-5:]:  # Show last 5 epochs\n",
    "        tqdm.write(f'{metrics[\"epoch\"]:<5} {metrics[\"train_loss\"]:<12.4f} {metrics[\"val_loss\"]:<12.4f} '\n",
    "                  f'{metrics[\"train_ssim\"]:<12.4f} {metrics[\"val_ssim\"]:<12.4f} {metrics[\"epoch_time\"]:<8.1f}s')\n",
    "\n",
    "print('Cell 8: train_model function definition completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 10: Starting infer_model function definition\n",
      "Cell 10: infer_model function definition completed\n"
     ]
    }
   ],
   "source": [
    "print('Cell 10: Starting infer_model function definition')\n",
    "# Inference function\n",
    "def infer_model():\n",
    "    print('Loading model for inference...')\n",
    "    # Load model\n",
    "    checkpoint = torch.load(config['save_path'])\n",
    "    model = UNetFusion().to(config['device'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print('Model loaded successfully')\n",
    "\n",
    "    # Load test dataset\n",
    "    test_dataset = LLVIPDataset('LLVIP', train=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    print(f'Test dataset size: {len(test_dataset)} samples')\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    print('Results directory created')\n",
    "\n",
    "    # Process test images with progress bar\n",
    "    print('Starting inference on test images...')\n",
    "    inference_bar = tqdm(test_loader,\n",
    "                        desc='ðŸ” Running Inference',\n",
    "                        unit='image',\n",
    "                        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',\n",
    "                        ncols=100, miniters=1, mininterval=0.1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (vis, ir, names) in enumerate(inference_bar):\n",
    "            vis = vis.to(config['device'])\n",
    "            ir = ir.to(config['device'])\n",
    "\n",
    "            fused = model(vis, ir)\n",
    "\n",
    "            # Convert to numpy for visualization\n",
    "            vis_img = vis.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "            ir_img = ir.squeeze().mean(0).cpu().numpy()  # Convert to grayscale\n",
    "            fused_img = fused.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "            # Plot results\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.subplot(131)\n",
    "            plt.imshow(vis_img)\n",
    "            plt.title('Visible (RGB)')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(132)\n",
    "            plt.imshow(ir_img, cmap='gray')\n",
    "            plt.title('Infrared')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(133)\n",
    "            plt.imshow(fused_img)\n",
    "            plt.title('Fused (RGB)')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.savefig(f'results/{names[0]}_fused.png', bbox_inches='tight', dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "            # Update progress bar with current image name\n",
    "            inference_bar.set_postfix({'image': names[0]}, refresh=True)\n",
    "\n",
    "            if i >= 4:  # Save only first 5 examples\n",
    "                break\n",
    "\n",
    "    tqdm.write('Inference completed. Results saved in results/ directory')\n",
    "\n",
    "print('Cell 10: infer_model function definition completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 11: Starting main execution\n",
      "Beginning training phase...\n",
      "Initializing training components...\n",
      "Initializing UNetFusion model...\n",
      "UNetFusion model initialized successfully\n",
      "UNetFusion model initialized successfully\n",
      "Found 12025 image pairs in train set\n",
      "SSIM initialized with window_size=11\n",
      "GradientLoss initialized\n",
      "Found 12025 image pairs in train set\n",
      "SSIM initialized with window_size=11\n",
      "GradientLoss initialized\n",
      "VGGPerceptual initialized on device: cuda\n",
      "FusionLoss initialized with weights: SSIM=1.0, Grad=1.0, Perc=0.1, Int=5.0\n",
      "Full dataset size: 12025 samples\n",
      "Training set size: 9620 samples\n",
      "Validation set size: 2405 samples\n",
      "Batch size: 8\n",
      "Number of training batches per epoch: 1203\n",
      "Number of validation batches per epoch: 301\n",
      "Total epochs: 2\n",
      "Starting training loop...\n",
      "VGGPerceptual initialized on device: cuda\n",
      "FusionLoss initialized with weights: SSIM=1.0, Grad=1.0, Perc=0.1, Int=5.0\n",
      "Full dataset size: 12025 samples\n",
      "Training set size: 9620 samples\n",
      "Validation set size: 2405 samples\n",
      "Batch size: 8\n",
      "Number of training batches per epoch: 1203\n",
      "Number of validation batches per epoch: 301\n",
      "Total epochs: 2\n",
      "Starting training loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Training Progress:   0%|                                                | 0/2 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Epoch 1/2 - Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Training Progress:   0%|                                                | 0/2 [04:45<?, ?epoch/s][00:00<?, ?batch/s] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoch 1/2 Summary (â±ï¸ 285.3s total):\n",
      "  ðŸŽ“ Training (â±ï¸ 255.0s):\n",
      "    Loss: 2.2509 | SSIM: 0.7201 | Grad: 0.3399 | Perc: 1.1716\n",
      "  âœ… Validation (â±ï¸ 30.3s):\n",
      "    Loss: 2.2569 | SSIM: 0.7222 | Grad: 0.3464 | Perc: 1.1819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Training Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 1/2 [04:46<04:46, 286.04s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Best model saved! (Val Loss: 2.2569)\n",
      "\n",
      "ðŸš€ Epoch 2/2 - Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [09:30<00:00, 285.08s/epoch][00:00<?, ?batch/s] \u001b[A\n",
      "/tmp/ipykernel_171427/109202486.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(config['save_path'])\n",
      "\n",
      "/tmp/ipykernel_171427/109202486.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(config['save_path'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoch 2/2 Summary (â±ï¸ 284.1s total):\n",
      "  ðŸŽ“ Training (â±ï¸ 254.1s):\n",
      "    Loss: 2.2516 | SSIM: 0.7202 | Grad: 0.3398 | Perc: 1.1715\n",
      "  âœ… Validation (â±ï¸ 30.0s):\n",
      "    Loss: 2.2587 | SSIM: 0.7226 | Grad: 0.3466 | Perc: 1.1824\n",
      "\n",
      "ðŸŽ‰ Training completed!\n",
      "ðŸ† Best validation loss: 2.2569\n",
      "ðŸ’¾ Final model saved to fusion_model_improved.pth\n",
      "\n",
      "ðŸ“ˆ Training History Summary:\n",
      "Epoch Train Loss   Val Loss     Train SSIM   Val SSIM     Time    \n",
      "--------------------------------------------------------------------------------\n",
      "1     2.2509       2.2569       0.7201       0.7222       285.3   s\n",
      "2     2.2516       2.2587       0.7202       0.7226       284.1   s\n",
      "Training phase completed.\n",
      "Beginning inference phase...\n",
      "Loading model for inference...\n",
      "Initializing UNetFusion model...\n",
      "UNetFusion model initialized successfully\n",
      "Model loaded successfully\n",
      "Found 3463 image pairs in test set\n",
      "Test dataset size: 3463 samples\n",
      "Results directory created\n",
      "Starting inference on test images...\n",
      "UNetFusion model initialized successfully\n",
      "Model loaded successfully\n",
      "Found 3463 image pairs in test set\n",
      "Test dataset size: 3463 samples\n",
      "Results directory created\n",
      "Starting inference on test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ” Running Inference:   0%|                                     | 4/3463 [00:01<23:40,  2.43image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed. Results saved in results/ directory\n",
      "Inference phase completed.\n",
      "All tasks completed successfully!\n",
      "Cell 11: Main execution completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Cell 11: Starting main execution')\n",
    "if __name__ == '__main__':\n",
    "    print('Beginning training phase...')\n",
    "    train_model()\n",
    "    print('Training phase completed.')\n",
    "    print('Beginning inference phase...')\n",
    "    infer_model()\n",
    "    print('Inference phase completed.')\n",
    "    print('All tasks completed successfully!')\n",
    "print('Cell 11: Main execution completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
