{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863f7f08",
   "metadata": {},
   "source": [
    "# Advanced Multi-Scale Attention-Guided Hybrid Fusion Strategy\n",
    "\n",
    "This notebook implements a novel fusion strategy that combines the best aspects of both existing approaches with state-of-the-art improvements.\n",
    "\n",
    "## Key Innovations:\n",
    "1. **Multi-Scale Wavelet Analysis** (3 scales instead of 1)\n",
    "2. **Attention-Guided Spatial Masks** (learning where to focus)\n",
    "3. **Hybrid Loss with Perceptual Component** (VGG-based perceptual loss)\n",
    "4. **Dynamic Frequency Weighting** (content-adaptive frequency selection)\n",
    "5. **Progressive Training Strategy** (curriculum learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1014d62",
   "metadata": {},
   "source": [
    "## 1. Import Enhanced Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876933f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Advanced Multi-Scale Fusion Network Defined\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_wavelets import DWTForward, DWTInverse\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced components for the hybrid approach\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel Attention Module for focusing on important frequency channels\"\"\"\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        attention = self.sigmoid(avg_out + max_out)\n",
    "        return x * attention\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial Attention Module for focusing on important spatial regions\"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        attention = torch.cat([avg_out, max_out], dim=1)\n",
    "        attention = self.sigmoid(self.conv(attention))\n",
    "        return x * attention\n",
    "\n",
    "class MultiScaleWaveletFusionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced Multi-Scale Attention-Guided Hybrid Fusion Network\n",
    "    \n",
    "    Combines:\n",
    "    - Multi-scale wavelet analysis (3 levels)\n",
    "    - Channel and spatial attention mechanisms\n",
    "    - Dynamic frequency weighting\n",
    "    - Content-adaptive fusion strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, wave='db4', num_scales=3):\n",
    "        super().__init__()\n",
    "        self.num_scales = num_scales\n",
    "        self.wave = wave\n",
    "        \n",
    "        # Multi-scale DWT/IDWT\n",
    "        self.dwt_levels = nn.ModuleList([\n",
    "            DWTForward(J=1, wave=wave) for _ in range(num_scales)\n",
    "        ])\n",
    "        self.idwt_levels = nn.ModuleList([\n",
    "            DWTInverse(wave=wave) for _ in range(num_scales)\n",
    "        ])\n",
    "        \n",
    "        # Enhanced mask networks for each scale\n",
    "        self.mask_networks = nn.ModuleList([\n",
    "            self._create_enhanced_mask_net() for _ in range(num_scales)\n",
    "        ])\n",
    "        \n",
    "        # Attention modules\n",
    "        self.channel_attention = nn.ModuleList([\n",
    "            ChannelAttention(4) for _ in range(num_scales)  # 4 frequency bands\n",
    "        ])\n",
    "        self.spatial_attention = nn.ModuleList([\n",
    "            SpatialAttention() for _ in range(num_scales)\n",
    "        ])\n",
    "        \n",
    "        # Dynamic frequency importance weights\n",
    "        self.frequency_importance = nn.ParameterList([\n",
    "            nn.Parameter(torch.ones(4)) for _ in range(num_scales)  # Low, LH, HL, HH\n",
    "        ])\n",
    "        \n",
    "        # Global fusion controller\n",
    "        self.global_fusion_net = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 32), nn.ReLU(),\n",
    "            nn.Linear(32, num_scales), nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def _create_enhanced_mask_net(self):\n",
    "        \"\"\"Create enhanced mask network with residual connections\"\"\"\n",
    "        return nn.Sequential(\n",
    "            # Initial feature extraction\n",
    "            nn.Conv2d(2, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            \n",
    "            # Residual block\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            \n",
    "            # Attention-guided refinement\n",
    "            nn.Conv2d(64, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            \n",
    "            # Output masks for 4 frequency bands\n",
    "            nn.Conv2d(16, 4, 3, padding=1), nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, ct, mri):\n",
    "        if ct.dim() == 3: ct = ct.unsqueeze(0)\n",
    "        if mri.dim() == 3: mri = mri.unsqueeze(0)\n",
    "        \n",
    "        batch_size = ct.shape[0]\n",
    "        device = ct.device\n",
    "        \n",
    "        # Global fusion weights from input analysis\n",
    "        input_concat = torch.cat([ct, mri], dim=1)\n",
    "        scale_weights = self.global_fusion_net(input_concat)  # (B, num_scales)\n",
    "        \n",
    "        # Multi-scale fusion\n",
    "        scale_results = []\n",
    "        scale_masks = []\n",
    "        \n",
    "        for scale_idx in range(self.num_scales):\n",
    "            # Apply appropriate downsampling for higher scales\n",
    "            if scale_idx > 0:\n",
    "                factor = 2 ** scale_idx\n",
    "                ct_scaled = F.avg_pool2d(ct, factor)\n",
    "                mri_scaled = F.avg_pool2d(mri, factor)\n",
    "            else:\n",
    "                ct_scaled = ct\n",
    "                mri_scaled = mri\n",
    "            \n",
    "            # Wavelet decomposition\n",
    "            ct_low, ct_high = self.dwt_levels[scale_idx](ct_scaled)\n",
    "            mri_low, mri_high = self.dwt_levels[scale_idx](mri_scaled)\n",
    "            \n",
    "            # Extract frequency components\n",
    "            ct_lh = ct_high[0][:, :, 0:1, :, :].squeeze(2)\n",
    "            ct_hl = ct_high[0][:, :, 1:2, :, :].squeeze(2)\n",
    "            ct_hh = ct_high[0][:, :, 2:3, :, :].squeeze(2)\n",
    "            \n",
    "            mri_lh = mri_high[0][:, :, 0:1, :, :].squeeze(2)\n",
    "            mri_hl = mri_high[0][:, :, 1:2, :, :].squeeze(2)\n",
    "            mri_hh = mri_high[0][:, :, 2:3, :, :].squeeze(2)\n",
    "            \n",
    "            # Generate spatial masks\n",
    "            low_stack = torch.cat([ct_low, mri_low], dim=1)\n",
    "            masks = self.mask_networks[scale_idx](low_stack)\n",
    "            \n",
    "            # Apply channel attention to masks\n",
    "            masks = self.channel_attention[scale_idx](masks)\n",
    "            \n",
    "            # Apply spatial attention\n",
    "            masks = self.spatial_attention[scale_idx](masks)\n",
    "            \n",
    "            # Extract individual masks\n",
    "            m_low = masks[:, 0:1]\n",
    "            m_lh = masks[:, 1:2] \n",
    "            m_hl = masks[:, 2:3]\n",
    "            m_hh = masks[:, 3:4]\n",
    "            \n",
    "            # Resize high-frequency masks if needed\n",
    "            if m_lh.shape[-2:] != ct_lh.shape[-2:]:\n",
    "                m_lh = F.interpolate(m_lh, size=ct_lh.shape[-2:], mode='bilinear', align_corners=False)\n",
    "                m_hl = F.interpolate(m_hl, size=ct_hl.shape[-2:], mode='bilinear', align_corners=False)\n",
    "                m_hh = F.interpolate(m_hh, size=ct_hh.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Apply dynamic frequency importance weighting\n",
    "            freq_weights = F.softmax(self.frequency_importance[scale_idx], dim=0)\n",
    "            \n",
    "            # Weighted fusion for each frequency band\n",
    "            fused_low = (m_low * freq_weights[0]) * ct_low + (1 - m_low * freq_weights[0]) * mri_low\n",
    "            fused_lh = (m_lh * freq_weights[1]) * ct_lh + (1 - m_lh * freq_weights[1]) * mri_lh\n",
    "            fused_hl = (m_hl * freq_weights[2]) * ct_hl + (1 - m_hl * freq_weights[2]) * mri_hl\n",
    "            fused_hh = (m_hh * freq_weights[3]) * ct_hh + (1 - m_hh * freq_weights[3]) * mri_hh\n",
    "            \n",
    "            # Reconstruct from wavelets\n",
    "            fused_high = torch.stack([fused_lh, fused_hl, fused_hh], dim=2)\n",
    "            fused_scale = self.idwt_levels[scale_idx]((fused_low, [fused_high]))\n",
    "            \n",
    "            # Resize to original size if needed\n",
    "            if scale_idx > 0:\n",
    "                fused_scale = F.interpolate(fused_scale, size=ct.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            scale_results.append(fused_scale)\n",
    "            scale_masks.append({\n",
    "                'low': m_low, 'lh': m_lh, 'hl': m_hl, 'hh': m_hh,\n",
    "                'freq_weights': freq_weights\n",
    "            })\n",
    "        \n",
    "        # Weighted combination of multi-scale results\n",
    "        final_result = torch.zeros_like(ct)\n",
    "        for scale_idx, scale_result in enumerate(scale_results):\n",
    "            weight = scale_weights[:, scale_idx:scale_idx+1, None, None]\n",
    "            final_result += weight * scale_result\n",
    "        \n",
    "        return final_result, {\n",
    "            'scale_weights': scale_weights,\n",
    "            'scale_masks': scale_masks,\n",
    "            'frequency_importance': [fw.data for fw in self.frequency_importance]\n",
    "        }\n",
    "\n",
    "print(\"âœ… Advanced Multi-Scale Fusion Network Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d5d14",
   "metadata": {},
   "source": [
    "## 2. Enhanced Loss Function with Perceptual Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ebe8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Advanced Loss Functions Defined\n"
     ]
    }
   ],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"VGG-based perceptual loss for better visual quality\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16(pretrained=True).features\n",
    "        self.vgg_layers = nn.Sequential(*list(vgg.children())[:16])  # Up to conv3_3\n",
    "        for param in self.vgg_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg_layers.eval()\n",
    "    \n",
    "    def forward(self, fused, ct, mri):\n",
    "        # Convert grayscale to RGB\n",
    "        fused_rgb = fused.repeat(1, 3, 1, 1)\n",
    "        ct_rgb = ct.repeat(1, 3, 1, 1)\n",
    "        mri_rgb = mri.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        # Extract features\n",
    "        fused_features = self.vgg_layers(fused_rgb)\n",
    "        ct_features = self.vgg_layers(ct_rgb)\n",
    "        mri_features = self.vgg_layers(mri_rgb)\n",
    "        \n",
    "        # Perceptual loss as average distance to both sources\n",
    "        loss_ct = F.mse_loss(fused_features, ct_features)\n",
    "        loss_mri = F.mse_loss(fused_features, mri_features)\n",
    "        \n",
    "        return (loss_ct + loss_mri) / 2\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    \"\"\"Improved SSIM Loss with multi-scale support\"\"\"\n",
    "    def __init__(self, window_size=11, size_average=True, val_range=1.0):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.val_range = val_range\n",
    "        self.register_buffer('window', self._create_window(window_size))\n",
    "    \n",
    "    def _gaussian(self, window_size, sigma):\n",
    "        gauss = torch.Tensor([np.exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "        return gauss/gauss.sum()\n",
    "    \n",
    "    def _create_window(self, window_size, channel=1):\n",
    "        _1D_window = self._gaussian(window_size, 1.5).unsqueeze(1)\n",
    "        _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "        window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "        return window\n",
    "    \n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "        window = self._create_window(self.window_size, channel).to(img1.device).type(img1.dtype)\n",
    "        return self._ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "    \n",
    "    def _ssim(self, img1, img2, window, window_size, channel, size_average=True):\n",
    "        mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)\n",
    "        mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)\n",
    "        \n",
    "        mu1_sq = mu1.pow(2)\n",
    "        mu2_sq = mu2.pow(2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "        \n",
    "        sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size//2, groups=channel) - mu1_sq\n",
    "        sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size//2, groups=channel) - mu2_sq\n",
    "        sigma12 = F.conv2d(img1 * img2, window, padding=window_size//2, groups=channel) - mu1_mu2\n",
    "        \n",
    "        C1 = (0.01 * self.val_range)**2\n",
    "        C2 = (0.03 * self.val_range)**2\n",
    "        \n",
    "        ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "        \n",
    "        if size_average:\n",
    "            return ssim_map.mean()\n",
    "        else:\n",
    "            return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "class GradientLoss(nn.Module):\n",
    "    \"\"\"Enhanced gradient loss with multi-directional gradients\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Sobel operators\n",
    "        self.sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        self.sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Prewitt operators for additional gradient information\n",
    "        self.prewitt_x = torch.tensor([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        self.prewitt_y = torch.tensor([[-1, -1, -1], [0, 0, 0], [1, 1, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def forward(self, fused, ct, mri):\n",
    "        device = fused.device\n",
    "        \n",
    "        # Move operators to device\n",
    "        sobel_x = self.sobel_x.to(device)\n",
    "        sobel_y = self.sobel_y.to(device)\n",
    "        prewitt_x = self.prewitt_x.to(device)\n",
    "        prewitt_y = self.prewitt_y.to(device)\n",
    "        \n",
    "        # Calculate gradients for all images\n",
    "        def calc_gradients(img):\n",
    "            gx_sobel = F.conv2d(img, sobel_x, padding=1)\n",
    "            gy_sobel = F.conv2d(img, sobel_y, padding=1)\n",
    "            gx_prewitt = F.conv2d(img, prewitt_x, padding=1)\n",
    "            gy_prewitt = F.conv2d(img, prewitt_y, padding=1)\n",
    "            \n",
    "            grad_sobel = torch.sqrt(gx_sobel**2 + gy_sobel**2 + 1e-8)\n",
    "            grad_prewitt = torch.sqrt(gx_prewitt**2 + gy_prewitt**2 + 1e-8)\n",
    "            \n",
    "            return (grad_sobel + grad_prewitt) / 2\n",
    "        \n",
    "        grad_fused = calc_gradients(fused)\n",
    "        grad_ct = calc_gradients(ct)\n",
    "        grad_mri = calc_gradients(mri)\n",
    "        \n",
    "        # Target gradient should preserve maximum edge information\n",
    "        grad_target = torch.maximum(grad_ct, grad_mri)\n",
    "        \n",
    "        return F.l1_loss(grad_fused, grad_target)\n",
    "\n",
    "class AdvancedFusionLoss(nn.Module):\n",
    "    \"\"\"Comprehensive loss function combining multiple objectives\"\"\"\n",
    "    def __init__(self, w_l1=1.0, w_ssim=2.0, w_grad=1.5, w_perceptual=0.5, w_freq=0.3):\n",
    "        super().__init__()\n",
    "        self.w_l1 = w_l1\n",
    "        self.w_ssim = w_ssim\n",
    "        self.w_grad = w_grad\n",
    "        self.w_perceptual = w_perceptual\n",
    "        self.w_freq = w_freq\n",
    "        \n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.ssim_loss = SSIM()\n",
    "        self.grad_loss = GradientLoss()\n",
    "        self.perceptual_loss = PerceptualLoss()\n",
    "    \n",
    "    def frequency_domain_loss(self, fused, ct, mri):\n",
    "        \"\"\"Loss in frequency domain to preserve spectral characteristics\"\"\"\n",
    "        # 2D FFT\n",
    "        fused_fft = torch.fft.fft2(fused)\n",
    "        ct_fft = torch.fft.fft2(ct)\n",
    "        mri_fft = torch.fft.fft2(mri)\n",
    "        \n",
    "        # Magnitude spectrum loss\n",
    "        fused_mag = torch.abs(fused_fft)\n",
    "        ct_mag = torch.abs(ct_fft)\n",
    "        mri_mag = torch.abs(mri_fft)\n",
    "        \n",
    "        # Preserve important frequency components from both sources\n",
    "        freq_loss = self.l1_loss(fused_mag, torch.maximum(ct_mag, mri_mag))\n",
    "        return freq_loss\n",
    "    \n",
    "    def forward(self, fused, ct, mri):\n",
    "        # Reconstruction losses\n",
    "        l1_ct = self.l1_loss(fused, ct)\n",
    "        l1_mri = self.l1_loss(fused, mri)\n",
    "        l1_total = l1_ct + l1_mri\n",
    "        \n",
    "        # Structural similarity losses\n",
    "        ssim_ct = 1 - self.ssim_loss(fused, ct)\n",
    "        ssim_mri = 1 - self.ssim_loss(fused, mri)\n",
    "        ssim_total = ssim_ct + ssim_mri\n",
    "        \n",
    "        # Edge preservation loss\n",
    "        grad_total = self.grad_loss(fused, ct, mri)\n",
    "        \n",
    "        # Perceptual loss\n",
    "        perceptual_total = self.perceptual_loss(fused, ct, mri)\n",
    "        \n",
    "        # Frequency domain loss\n",
    "        freq_total = self.frequency_domain_loss(fused, ct, mri)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = (self.w_l1 * l1_total + \n",
    "                     self.w_ssim * ssim_total + \n",
    "                     self.w_grad * grad_total + \n",
    "                     self.w_perceptual * perceptual_total +\n",
    "                     self.w_freq * freq_total)\n",
    "        \n",
    "        return total_loss, {\n",
    "            'total': total_loss,\n",
    "            'l1': l1_total,\n",
    "            'ssim': ssim_total,\n",
    "            'gradient': grad_total,\n",
    "            'perceptual': perceptual_total,\n",
    "            'frequency': freq_total\n",
    "        }\n",
    "\n",
    "print(\"âœ… Advanced Loss Functions Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ed783",
   "metadata": {},
   "source": [
    "## 3. Training Strategy with Progressive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b5e950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Progressive Training Strategy Defined\n"
     ]
    }
   ],
   "source": [
    "class ProgressiveTrainingScheduler:\n",
    "    \"\"\"Progressive training strategy that gradually increases complexity\"\"\"\n",
    "    \n",
    "    def __init__(self, total_epochs=50):\n",
    "        self.total_epochs = total_epochs\n",
    "        self.phase_epochs = total_epochs // 3\n",
    "        \n",
    "    def get_loss_weights(self, epoch):\n",
    "        \"\"\"Dynamically adjust loss weights during training\"\"\"\n",
    "        if epoch < self.phase_epochs:\n",
    "            # Phase 1: Focus on basic reconstruction\n",
    "            return {'w_l1': 2.0, 'w_ssim': 1.0, 'w_grad': 0.5, 'w_perceptual': 0.1, 'w_freq': 0.1}\n",
    "        elif epoch < 2 * self.phase_epochs:\n",
    "            # Phase 2: Add structural and edge preservation\n",
    "            return {'w_l1': 1.5, 'w_ssim': 2.0, 'w_grad': 1.5, 'w_perceptual': 0.3, 'w_freq': 0.2}\n",
    "        else:\n",
    "            # Phase 3: Full perceptual and frequency awareness\n",
    "            return {'w_l1': 1.0, 'w_ssim': 2.0, 'w_grad': 1.5, 'w_perceptual': 0.8, 'w_freq': 0.5}\n",
    "    \n",
    "    def get_learning_rate(self, base_lr, epoch):\n",
    "        \"\"\"Cosine annealing learning rate schedule\"\"\"\n",
    "        return base_lr * 0.5 * (1 + np.cos(np.pi * epoch / self.total_epochs))\n",
    "\n",
    "def train_advanced_fusion_model(model, train_loader, val_loader=None, epochs=50, base_lr=1e-4):\n",
    "    \"\"\"Advanced training function with progressive learning\"\"\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    scheduler = ProgressiveTrainingScheduler(epochs)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=1e-5)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'epoch_losses': [],\n",
    "        'loss_components': [],\n",
    "        'learning_rates': [],\n",
    "        'validation_scores': []\n",
    "    }\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    print(\"ðŸš€ Starting Advanced Progressive Training\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Get progressive loss weights and learning rate\n",
    "        loss_weights = scheduler.get_loss_weights(epoch)\n",
    "        current_lr = scheduler.get_learning_rate(base_lr, epoch)\n",
    "        \n",
    "        # Update learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = current_lr\n",
    "        \n",
    "        # Create criterion with current weights\n",
    "        criterion = AdvancedFusionLoss(**loss_weights)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        epoch_components = []\n",
    "        \n",
    "        print(f\"\\\\nEpoch [{epoch+1}/{epochs}] - LR: {current_lr:.6f}\")\n",
    "        print(f\"Loss weights: {loss_weights}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for batch_idx, (ct_batch, mri_batch) in enumerate(train_loader):\n",
    "            ct_batch = ct_batch.to(device)\n",
    "            mri_batch = mri_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            fused_batch, fusion_info = model(ct_batch, mri_batch)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss, loss_components = criterion(fused_batch, ct_batch, mri_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Record losses\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_components.append({k: v.item() for k, v in loss_components.items()})\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Batch [{batch_idx:3d}] | \"\n",
    "                      f\"Loss: {loss.item():.4f} | \"\n",
    "                      f\"L1: {loss_components['l1'].item():.4f} | \"\n",
    "                      f\"SSIM: {loss_components['ssim'].item():.4f} | \"\n",
    "                      f\"Grad: {loss_components['gradient'].item():.4f}\")\n",
    "        \n",
    "        # Calculate epoch averages\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        avg_components = {}\n",
    "        for key in epoch_components[0].keys():\n",
    "            avg_components[key] = np.mean([comp[key] for comp in epoch_components])\n",
    "        \n",
    "        # Validation (if available)\n",
    "        val_score = None\n",
    "        if val_loader is not None:\n",
    "            val_score = validate_model(model, val_loader, device)\n",
    "            print(f\"  Validation SSIM: {val_score:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_epoch = epoch + 1\n",
    "            \n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_loss': best_loss,\n",
    "                'loss_weights': loss_weights,\n",
    "                'validation_score': val_score\n",
    "            }\n",
    "            \n",
    "            os.makedirs('checkpoints_advanced', exist_ok=True)\n",
    "            torch.save(checkpoint, 'checkpoints_advanced/advanced_fusion_best.pt')\n",
    "            print(f\"  âœ… New best model saved! (Loss: {best_loss:.4f})\")\n",
    "        \n",
    "        # Record history\n",
    "        history['epoch_losses'].append(avg_loss)\n",
    "        history['loss_components'].append(avg_components)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        history['validation_scores'].append(val_score)\n",
    "        \n",
    "        # Display epoch summary\n",
    "        print(f\"  Epoch Summary - Total: {avg_loss:.4f} | \"\n",
    "              f\"L1: {avg_components['l1']:.4f} | \"\n",
    "              f\"SSIM: {avg_components['ssim']:.4f} | \"\n",
    "              f\"Perceptual: {avg_components['perceptual']:.4f}\")\n",
    "    \n",
    "    print(f\"\\\\n\" + \"=\" * 60)\n",
    "    print(f\"ðŸŽ¯ Training Complete!\")\n",
    "    print(f\"   Best epoch: {best_epoch}\")\n",
    "    print(f\"   Best loss: {best_loss:.4f}\")\n",
    "    print(f\"   Model saved: checkpoints_advanced/advanced_fusion_best.pt\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "def validate_model(model, val_loader, device):\n",
    "    \"\"\"Validation function to compute SSIM score\"\"\"\n",
    "    model.eval()\n",
    "    ssim_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ct_batch, mri_batch in val_loader:\n",
    "            ct_batch = ct_batch.to(device)\n",
    "            mri_batch = mri_batch.to(device)\n",
    "            \n",
    "            fused_batch, _ = model(ct_batch, mri_batch)\n",
    "            \n",
    "            # Calculate SSIM for each image in batch\n",
    "            for i in range(fused_batch.shape[0]):\n",
    "                ssim_ct = ssim(fused_batch[i].cpu().numpy().squeeze(), \n",
    "                              ct_batch[i].cpu().numpy().squeeze(), data_range=1.0)\n",
    "                ssim_mri = ssim(fused_batch[i].cpu().numpy().squeeze(), \n",
    "                               mri_batch[i].cpu().numpy().squeeze(), data_range=1.0)\n",
    "                ssim_scores.append((ssim_ct + ssim_mri) / 2)\n",
    "    \n",
    "    return np.mean(ssim_scores)\n",
    "\n",
    "print(\"âœ… Progressive Training Strategy Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d06b06",
   "metadata": {},
   "source": [
    "## 4. Key Advantages of This Novel Approach\n",
    "\n",
    "### ðŸŽ¯ **Strategic Advantages:**\n",
    "\n",
    "1. **Multi-Scale Analysis** - Captures details at different resolutions\n",
    "2. **Attention Mechanisms** - Focuses on important regions and channels\n",
    "3. **Perceptual Quality** - VGG-based loss for better visual results\n",
    "4. **Progressive Learning** - Curriculum-based training for optimal convergence\n",
    "5. **Dynamic Weighting** - Content-adaptive fusion strategies\n",
    "6. **Frequency Awareness** - Preserves important spectral characteristics\n",
    "\n",
    "### ðŸ“Š **Expected Performance Improvements:**\n",
    "\n",
    "| Metric | Current Best | Expected | Improvement |\n",
    "|--------|-------------|----------|-------------|\n",
    "| **SSIM** | 0.73 | **0.82-0.88** | **+12-21%** |\n",
    "| **PSNR** | 18.6 | **22-26** | **+18-40%** |\n",
    "| **Edge Preservation** | 0.92 | **0.95-0.98** | **+3-7%** |\n",
    "| **Perceptual Quality** | N/A | **Significantly Better** | **New Metric** |\n",
    "\n",
    "### ðŸ”¬ **Technical Innovations:**\n",
    "\n",
    "- **Attention-Guided Masks**: Learn to focus on important spatial and channel features\n",
    "- **Multi-Scale Fusion**: Combine information from multiple frequency scales\n",
    "- **Perceptual Loss**: Better visual quality using pre-trained VGG features\n",
    "- **Progressive Training**: Gradually increase complexity for better convergence\n",
    "- **Frequency Domain Loss**: Preserve spectral characteristics\n",
    "\n",
    "### ðŸ’¡ **Implementation Strategy:**\n",
    "\n",
    "1. **Start with Progressive Training** - Begin with basic reconstruction, add complexity\n",
    "2. **Multi-Scale Validation** - Test on different image sizes and content types\n",
    "3. **Ablation Studies** - Verify each component's contribution\n",
    "4. **Real-World Testing** - Validate on clinical datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184b30f0",
   "metadata": {},
   "source": [
    "## 5. Usage Example\n",
    "\n",
    "```python\n",
    "# Initialize the advanced model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultiScaleWaveletFusionNet(wave='db4', num_scales=3)\n",
    "\n",
    "# Create dataset and data loader\n",
    "dataset = CTMRIDataset('path/to/ct', 'path/to/mri')\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "history = train_advanced_fusion_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    epochs=50,\n",
    "    base_lr=1e-4\n",
    ")\n",
    "\n",
    "# The model will be automatically saved as 'checkpoints_advanced/advanced_fusion_best.pt'\n",
    "```\n",
    "\n",
    "### ðŸš€ **Next Steps:**\n",
    "1. Implement this advanced architecture\n",
    "2. Train on your CT-MRI dataset\n",
    "3. Compare against existing Option 1 and Option 2 models\n",
    "4. Fine-tune hyperparameters based on results\n",
    "5. Deploy for clinical evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
