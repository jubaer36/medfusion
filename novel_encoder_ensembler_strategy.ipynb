{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fusion_pipeline.py\n",
    "\n",
    "A three-stage medical image fusion pipeline for multi-modal medical imaging (e.g., CT, MRI, PET).\n",
    "\n",
    "Stage 1: Hybrid CNN + Transformer Autoencoder (unsupervised per modality)\n",
    "Stage 2: Ensemble Feature Fusion (concat + conv / cross-attention / gated-weighted)\n",
    "Stage 3: Post-Fusion Reconstruction & Enhancement (U-Net style decoder + optional enhancement)\n",
    "\n",
    "Designed for small datasets: supports self-supervised pretraining, strong augmentation,\n",
    "transfer learning via adapter layers, mixed precision, k-fold CV, and early stopping.\n",
    "\n",
    "Author: ChatGPT (GPT-5 Thinking)\n",
    "Date: 2025-08-23\n",
    "License: MIT\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3edea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Utilities\n",
    "# ==========================\n",
    "\n",
    "def seed_all(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def num_params(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e883f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================\n",
    "# Positional Encoding (2D -> sequence)\n",
    "# ==========================\n",
    "class SinusoidalPositionalEncoding2D(nn.Module):\n",
    "    \"\"\"2D sine-cosine positional encoding for flattened feature maps.\n",
    "    Produces [B, H*W, C] additive encodings.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        assert channels % 4 == 0, \"channels must be divisible by 4\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, C, H, W]\n",
    "        b, c, h, w = x.shape\n",
    "        device = x.device\n",
    "        y_pos = torch.arange(h, device=device).unsqueeze(1).repeat(1, w)\n",
    "        x_pos = torch.arange(w, device=device).unsqueeze(0).repeat(h, 1)\n",
    "\n",
    "        dim_t = torch.arange(self.channels // 4, device=device).float()\n",
    "        dim_t = 10000 ** (2 * (dim_t // 2) / (self.channels // 2))\n",
    "\n",
    "        pe_y = y_pos[..., None] / dim_t\n",
    "        pe_x = x_pos[..., None] / dim_t\n",
    "        pe = torch.cat([torch.sin(pe_y), torch.cos(pe_y), torch.sin(pe_x), torch.cos(pe_x)], dim=-1)\n",
    "        pe = pe.view(h * w, self.channels)\n",
    "        pe = pe.unsqueeze(0).repeat(b, 1, 1)  # [B, H*W, C]\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f94dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Core building blocks\n",
    "# ==========================\n",
    "class ConvBNAct(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=3, s=1, p=1, act=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, k, s, p, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.act = nn.ReLU(inplace=True) if act else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBNAct(ch, ch, 3, 1, 1),\n",
    "            ConvBNAct(ch, ch, 3, 1, 1, act=False),\n",
    "        )\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.block(x) + x)\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Conv patch embedding to sequence tokens.\"\"\"\n",
    "    def __init__(self, in_ch: int, embed_dim: int, patch_size: int = 4):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, D, H', W']\n",
    "        b, d, h, w = x.shape\n",
    "        x_flat = x.flatten(2).transpose(1, 2)  # [B, N, D]\n",
    "        return x, x_flat, (h, w)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim: int, depth: int = 4, nheads: int = 8, mlp_ratio: float = 4.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nheads,\n",
    "                                                   dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "                                                   dropout=dropout, activation='gelu', batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        # x_seq: [B, N, D]\n",
    "        return self.encoder(x_seq)\n",
    "\n",
    "\n",
    "class SimpleSelfAttention2D(nn.Module):\n",
    "    \"\"\"Lightweight spatial attention over 2D feature maps.\"\"\"\n",
    "    def __init__(self, ch, reduction=8):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch // reduction, ch, 1, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.mlp(self.avg(x))\n",
    "        return x * w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc925ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Stage 1: Hybrid Encoder & Autoencoder\n",
    "# ==========================\n",
    "class HybridEncoder(nn.Module):\n",
    "    \"\"\"CNN stem -> Transformer encoder. Returns multi-scale features for skip connections.\"\"\"\n",
    "    def __init__(self, in_ch: int = 1, base_ch: int = 32, embed_dim: int = 256, trans_depth: int = 4, heads: int = 8):\n",
    "        super().__init__()\n",
    "        # CNN stem\n",
    "        self.s1 = nn.Sequential(ConvBNAct(in_ch, base_ch, 3, 1, 1), ResidualBlock(base_ch))\n",
    "        self.down1 = ConvBNAct(base_ch, base_ch * 2, 3, 2, 1)  # /2\n",
    "        self.s2 = ResidualBlock(base_ch * 2)\n",
    "        self.down2 = ConvBNAct(base_ch * 2, base_ch * 4, 3, 2, 1)  # /4\n",
    "        self.s3 = ResidualBlock(base_ch * 4)\n",
    "\n",
    "        # Patch embedding for transformer at lowest resolution\n",
    "        self.patch = PatchEmbed(base_ch * 4, embed_dim, patch_size=1)  # keep resolution\n",
    "        self.pos = SinusoidalPositionalEncoding2D(embed_dim)\n",
    "        self.trans = TransformerEncoder(embed_dim, depth=trans_depth, nheads=heads)\n",
    "\n",
    "        self.out_channels = {\n",
    "            's1': base_ch, 's2': base_ch * 2, 's3': base_ch * 4, 'token': embed_dim\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1 = self.s1(x)\n",
    "        s2 = self.s2(self.down1(s1))\n",
    "        s3 = self.s3(self.down2(s2))\n",
    "        fmap, seq, (h, w) = self.patch(s3)\n",
    "        seq = seq + self.pos(fmap)\n",
    "        tokens = self.trans(seq)  # [B, H*W, D]\n",
    "        tokens_2d = tokens.transpose(1, 2).view(x.size(0), -1, h, w)\n",
    "        return {\n",
    "            's1': s1, 's2': s2, 's3': s3, 'token': tokens_2d\n",
    "        }\n",
    "\n",
    "\n",
    "class HybridAutoencoder(nn.Module):\n",
    "    def __init__(self, in_ch=1, base_ch=32, embed_dim=256, trans_depth=4, heads=8):\n",
    "        super().__init__()\n",
    "        self.encoder = HybridEncoder(in_ch, base_ch, embed_dim, trans_depth, heads)\n",
    "        ch = self.encoder.out_channels\n",
    "        # Decoder (U-Net style)\n",
    "        self.up2 = nn.ConvTranspose2d(ch['token'], ch['s2'], 2, 2)\n",
    "        self.dec2 = nn.Sequential(ConvBNAct(ch['s2'] + ch['s2'], ch['s2']), ResidualBlock(ch['s2']))\n",
    "        self.up1 = nn.ConvTranspose2d(ch['s2'], ch['s1'], 2, 2)\n",
    "        self.dec1 = nn.Sequential(ConvBNAct(ch['s1'] + ch['s1'], ch['s1']), ResidualBlock(ch['s1']))\n",
    "        self.out = nn.Conv2d(ch['s1'], in_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.encoder(x)\n",
    "        x2 = self.up2(feats['token'])\n",
    "        x2 = self.dec2(torch.cat([x2, feats['s2']], dim=1))\n",
    "        x1 = self.up1(x2)\n",
    "        x1 = self.dec1(torch.cat([x1, feats['s1']], dim=1))\n",
    "        out = self.out(x1)\n",
    "        return out, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e0f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================\n",
    "# Stage 2: Ensemble Feature Fusion\n",
    "# ==========================\n",
    "class GatedWeightedFusion(nn.Module):\n",
    "    \"\"\"Learned per-channel weights for two or more feature maps of equal shape.\"\"\"\n",
    "    def __init__(self, ch: int, n_inputs: int = 2):\n",
    "        super().__init__()\n",
    "        self.gates = nn.Parameter(torch.zeros(n_inputs, ch))\n",
    "        nn.init.normal_(self.gates, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]):\n",
    "        # features: list of [B, C, H, W]\n",
    "        assert len(features) == self.gates.shape[0]\n",
    "        weights = F.softmax(self.gates, dim=0)  # [n, C]\n",
    "        fused = 0\n",
    "        for i, f in enumerate(features):\n",
    "            w = weights[i].view(1, -1, 1, 1)\n",
    "            fused = fused + f * w\n",
    "        return fused\n",
    "\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"Cross-attention between modality tokens; returns fused tokens as 2D map.\"\"\"\n",
    "    def __init__(self, ch: int, heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (ch // heads) ** -0.5\n",
    "        self.q_proj = nn.Conv2d(ch, ch, 1)\n",
    "        self.k_proj = nn.Conv2d(ch, ch, 1)\n",
    "        self.v_proj = nn.Conv2d(ch, ch, 1)\n",
    "        self.out = nn.Conv2d(ch, ch, 1)\n",
    "\n",
    "    def forward(self, a: torch.Tensor, b: torch.Tensor):\n",
    "        # a, b: [B, C, H, W]\n",
    "        q = self.q_proj(a)\n",
    "        k = self.k_proj(b)\n",
    "        v = self.v_proj(b)\n",
    "        bsz, c, h, w = q.shape\n",
    "        q = q.view(bsz, self.heads, c // self.heads, h * w)\n",
    "        k = k.view(bsz, self.heads, c // self.heads, h * w)\n",
    "        v = v.view(bsz, self.heads, c // self.heads, h * w)\n",
    "        attn = torch.einsum('bhcd,bhce->bhde', q, k) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = torch.einsum('bhde,bhce->bhcd', attn, v)\n",
    "        out = out.reshape(bsz, c, h, w)\n",
    "        return self.out(out)\n",
    "\n",
    "\n",
    "class FusionBlock(nn.Module):\n",
    "    \"\"\"Compose multiple fusion strategies.\n",
    "    mode in {\"concat_conv\", \"gated\", \"cross_attn\"}\n",
    "    \"\"\"\n",
    "    def __init__(self, ch_in: List[int], mode: str = \"concat_conv\"):\n",
    "        super().__init__()\n",
    "        assert len(set(ch_in)) == 1, \"All input feature maps must have same channel count\"\n",
    "        ch = ch_in[0]\n",
    "        self.mode = mode\n",
    "        if mode == \"concat_conv\":\n",
    "            self.block = nn.Sequential(\n",
    "                ConvBNAct(ch * len(ch_in), ch),\n",
    "                ResidualBlock(ch),\n",
    "                SimpleSelfAttention2D(ch)\n",
    "            )\n",
    "        elif mode == \"gated\":\n",
    "            self.block = nn.Sequential(GatedWeightedFusion(ch, n_inputs=len(ch_in)), ResidualBlock(ch))\n",
    "        elif mode == \"cross_attn\":\n",
    "            assert len(ch_in) == 2, \"cross_attn currently supports exactly two inputs\"\n",
    "            self.cross = CrossAttentionFusion(ch)\n",
    "            self.post = nn.Sequential(ResidualBlock(ch), SimpleSelfAttention2D(ch))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown fusion mode\")\n",
    "\n",
    "    def forward(self, feats: List[torch.Tensor]):\n",
    "        if self.mode == \"concat_conv\":\n",
    "            x = torch.cat(feats, dim=1)\n",
    "            return self.block(x)\n",
    "        elif self.mode == \"gated\":\n",
    "            return self.block[0](feats) if isinstance(self.block, nn.Sequential) else self.block(feats)\n",
    "        elif self.mode == \"cross_attn\":\n",
    "            x = self.cross(feats[0], feats[1])\n",
    "            return self.post(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e14724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Stage 3: Decoder + Enhancement\n",
    "# ==========================\n",
    "class UnsharpMask(nn.Module):\n",
    "    def __init__(self, kernel_size=5, amount=0.5):\n",
    "        super().__init__()\n",
    "        self.pad = nn.ReflectionPad2d(kernel_size // 2)\n",
    "        self.blur = nn.Conv2d(1, 1, kernel_size, padding=0, bias=False, groups=1)\n",
    "        with torch.no_grad():\n",
    "            # simple normalized blur kernel\n",
    "            k = torch.ones(1, 1, kernel_size, kernel_size) / (kernel_size ** 2)\n",
    "            self.blur.weight.copy_(k)\n",
    "        self.amount = amount\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.blur(self.pad(x))\n",
    "        return x + self.amount * (x - y)\n",
    "\n",
    "\n",
    "class FusionDecoder(nn.Module):\n",
    "    def __init__(self, ch: Dict[str, int], out_ch: int = 1):\n",
    "        super().__init__()\n",
    "        self.up2 = nn.ConvTranspose2d(ch['token'], ch['s2'], 2, 2)\n",
    "        self.dec2 = nn.Sequential(ConvBNAct(ch['s2'] + ch['s2'], ch['s2']), ResidualBlock(ch['s2']))\n",
    "        self.up1 = nn.ConvTranspose2d(ch['s2'], ch['s1'], 2, 2)\n",
    "        self.dec1 = nn.Sequential(ConvBNAct(ch['s1'] + ch['s1'], ch['s1']), ResidualBlock(ch['s1']))\n",
    "        self.out = nn.Conv2d(ch['s1'], out_ch, 1)\n",
    "        self.enhance = UnsharpMask(kernel_size=7, amount=0.3)\n",
    "\n",
    "    def forward(self, fused_feats: Dict[str, torch.Tensor], skip: Dict[str, torch.Tensor]):\n",
    "        x2 = self.up2(fused_feats['token'])\n",
    "        x2 = self.dec2(torch.cat([x2, skip['s2']], dim=1))\n",
    "        x1 = self.up1(x2)\n",
    "        x1 = self.dec1(torch.cat([x1, skip['s1']], dim=1))\n",
    "        out = self.out(x1)\n",
    "        out = self.enhance(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FullFusionModel(nn.Module):\n",
    "    \"\"\"Encoders per modality -> fusion on deepest token map -> decoder to fused image.\n",
    "    Optionally, auxiliary decoders reconstruct individual modalities to stabilize training.\n",
    "    \"\"\"\n",
    "    def __init__(self, modalities: List[str] = [\"ct\", \"mri\"], in_ch_map: Optional[Dict[str, int]] = None,\n",
    "                 base_ch=32, embed_dim=256, trans_depth=4, heads=8, fusion_mode=\"concat_conv\", aux_decoders=True):\n",
    "        super().__init__()\n",
    "        self.modalities = modalities\n",
    "        if in_ch_map is None:\n",
    "            in_ch_map = {m: 1 for m in modalities}\n",
    "        self.encoders = nn.ModuleDict({m: HybridEncoder(in_ch_map[m], base_ch, embed_dim, trans_depth, heads) for m in modalities})\n",
    "        ch = next(iter(self.encoders.values())).out_channels\n",
    "\n",
    "        self.fusion = FusionBlock([ch['token'] for _ in modalities], mode=fusion_mode)\n",
    "        self.decoder = FusionDecoder(ch, out_ch=1)\n",
    "        self.aux_decoders = nn.ModuleDict() if aux_decoders else None\n",
    "        if aux_decoders:\n",
    "            for m in modalities:\n",
    "                self.aux_decoders[m] = FusionDecoder(ch, out_ch=in_ch_map[m])\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]):\n",
    "        # inputs: dict {modality: tensor [B, C, H, W]}\n",
    "        enc_feats = {m: self.encoders[m](inputs[m]) for m in self.modalities}\n",
    "        # collect deepest tokens for fusion\n",
    "        tokens = [enc_feats[m]['token'] for m in self.modalities]\n",
    "        fused_token = self.fusion(tokens)\n",
    "        fused_feats = {\"token\": fused_token}\n",
    "        # use skip connections from first modality as spatial scaffold\n",
    "        scaffold = enc_feats[self.modalities[0]]\n",
    "        fused_img = self.decoder(fused_feats, skip=scaffold)\n",
    "\n",
    "        aux_outs = {}\n",
    "        if self.aux_decoders is not None:\n",
    "            for m in self.modalities:\n",
    "                aux_outs[m] = self.aux_decoders[m](fused_feats, skip=enc_feats[m])\n",
    "        return fused_img, enc_feats, aux_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb57ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Losses\n",
    "# ==========================\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self, window_size: int = 11, C1: float = 0.01**2, C2: float = 0.03**2):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        self.pad = nn.ReflectionPad2d(window_size // 2)\n",
    "        self.avg = nn.Conv2d(1, 1, window_size, bias=False, groups=1)\n",
    "        with torch.no_grad():\n",
    "            self.avg.weight.fill_(1.0 / (window_size ** 2))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mu_x = self.avg(self.pad(x))\n",
    "        mu_y = self.avg(self.pad(y))\n",
    "        sigma_x = self.avg(self.pad(x * x)) - mu_x * mu_x\n",
    "        sigma_y = self.avg(self.pad(y * y)) - mu_y * mu_y\n",
    "        sigma_xy = self.avg(self.pad(x * y)) - mu_x * mu_y\n",
    "        ssim = ((2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)) / ((mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2))\n",
    "        return 1 - ssim.mean()\n",
    "\n",
    "\n",
    "class EdgeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        kx = torch.tensor([[1, 0, -1],[2, 0, -2],[1, 0, -1]], dtype=torch.float32).view(1,1,3,3)\n",
    "        ky = torch.tensor([[1, 2, 1],[0, 0, 0],[-1,-2,-1]], dtype=torch.float32).view(1,1,3,3)\n",
    "        self.register_buffer('kx', kx)\n",
    "        self.register_buffer('ky', ky)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        gx_x = F.conv2d(x, self.kx, padding=1)\n",
    "        gx_y = F.conv2d(x, self.ky, padding=1)\n",
    "        gy_x = F.conv2d(y, self.kx, padding=1)\n",
    "        gy_y = F.conv2d(y, self.ky, padding=1)\n",
    "        ex = torch.sqrt(gx_x**2 + gx_y**2 + 1e-6)\n",
    "        ey = torch.sqrt(gy_x**2 + gy_y**2 + 1e-6)\n",
    "        return F.l1_loss(ex, ey)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LossWeights:\n",
    "    recon_l1: float = 1.0\n",
    "    recon_ssim: float = 1.0\n",
    "    edge: float = 0.5\n",
    "    aux_recon: float = 0.5\n",
    "    modality_consistency: float = 0.2\n",
    "\n",
    "\n",
    "class FusionLoss(nn.Module):\n",
    "    \"\"\"No-reference fused supervision + reconstruction heads to each modality.\n",
    "    - Encourage fused image to be structurally similar to max-information of inputs: use SSIM/L1 vs. per-modality inputs aggregated by saliency weights.\n",
    "    - Auxiliary decoders reconstruct each input modality (stabilizes learning on small datasets).\n",
    "    - Consistency: fused low-level gradients close to max of input gradients.\n",
    "    \"\"\"\n",
    "    def __init__(self, weights: LossWeights):\n",
    "        super().__init__()\n",
    "        self.w = weights\n",
    "        self.ssim = SSIMLoss()\n",
    "        self.edge = EdgeLoss()\n",
    "\n",
    "    def forward(self, fused: torch.Tensor, inputs: Dict[str, torch.Tensor], aux_outs: Dict[str, torch.Tensor]):\n",
    "        # Build a pseudo-target by weighted combination of inputs using gradient magnitude as saliency\n",
    "        with torch.no_grad():\n",
    "            mags = []\n",
    "            for x in inputs.values():\n",
    "                gx = F.conv2d(x, torch.tensor([[[[1,0,-1],[2,0,-2],[1,0,-1]]]], device=x.device), padding=1)\n",
    "                gy = F.conv2d(x, torch.tensor([[[[1,2,1],[0,0,0],[-1,-2,-1]]]], device=x.device), padding=1)\n",
    "                mags.append(torch.sqrt(gx**2 + gy**2 + 1e-6))\n",
    "            sal = torch.stack(mags, dim=0)  # [M, B, 1, H, W]\n",
    "            w = torch.softmax(sal, dim=0)\n",
    "            pseudo = (w * torch.stack(list(inputs.values()), dim=0)).sum(dim=0)\n",
    "\n",
    "        l1 = F.l1_loss(fused, pseudo)\n",
    "        ssim = self.ssim(fused, pseudo)\n",
    "        edge = self.edge(fused, pseudo)\n",
    "        loss = self.w.recon_l1 * l1 + self.w.recon_ssim * ssim + self.w.edge * edge\n",
    "\n",
    "        if aux_outs:\n",
    "            aux_loss = 0\n",
    "            for m, out in aux_outs.items():\n",
    "                aux_loss += F.l1_loss(out, inputs[m]) + self.ssim(out, inputs[m])\n",
    "            loss = loss + self.w.aux_recon * aux_loss / max(1, len(aux_outs))\n",
    "\n",
    "        # Consistency: fused gradient close to elementwise max of input gradients\n",
    "        with torch.no_grad():\n",
    "            grad_max, _ = torch.max(torch.stack(mags, dim=0), dim=0)\n",
    "        grad_cons = self.edge(fused, grad_max)\n",
    "        loss = loss + self.w.modality_consistency * grad_cons\n",
    "        metrics = {\n",
    "            'l1': l1.item(), 'ssim': 1 - ssim.item(), 'edge': edge.item()\n",
    "        }\n",
    "        return loss, metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Dataset skeleton (expects pre-registered pairs)\n",
    "# ==========================\n",
    "class PairedFusionDataset(Dataset):\n",
    "    def __init__(self, pairs: List[Dict[str, str]], modalities: List[str], transform=None):\n",
    "        \"\"\"\n",
    "        pairs: list of dicts mapping modality -> file path per sample, e.g.,\n",
    "            {'ct': 'path/ct.png', 'mri': 'path/mri.png'}\n",
    "        Images must be co-registered and same size. Implement your own loader.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pairs = pairs\n",
    "        self.modalities = modalities\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def _load_img(self, path: str) -> np.ndarray:\n",
    "        # Placeholder: user should replace with actual medical image loader (nibabel for NIfTI, SimpleITK/DICOM, etc.)\n",
    "        from PIL import Image\n",
    "        img = Image.open(path).convert('L')\n",
    "        return np.array(img, dtype=np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.pairs[idx]\n",
    "        data = {}\n",
    "        for m in self.modalities:\n",
    "            arr = self._load_img(item[m])\n",
    "            arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-6)  # min-max\n",
    "            t = torch.from_numpy(arr)[None, ...]  # [1,H,W]\n",
    "            data[m] = t\n",
    "        if self.transform:\n",
    "            # Apply same spatial transform to all modalities\n",
    "            seed = np.random.randint(0, 1e9)\n",
    "            torch.manual_seed(seed)\n",
    "            for m in self.modalities:\n",
    "                data[m] = self.transform(data[m])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24933bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Training routines\n",
    "# ==========================\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    modalities: List[str]\n",
    "    batch_size: int = 4\n",
    "    lr: float = 1e-4\n",
    "    epochs_stage1: int = 30\n",
    "    epochs_stage23: int = 60\n",
    "    fusion_mode: str = \"concat_conv\"\n",
    "    amp: bool = True\n",
    "    grad_accum: int = 1\n",
    "    weight_decay: float = 1e-4\n",
    "\n",
    "\n",
    "def pretrain_autoencoders(dataloader: DataLoader, modalities: List[str], device: str = 'cuda') -> Dict[str, HybridAutoencoder]:\n",
    "    models = {m: HybridAutoencoder(in_ch=1).to(device) for m in modalities}\n",
    "    opts = {m: torch.optim.AdamW(models[m].parameters(), lr=1e-4, weight_decay=1e-4) for m in modalities}\n",
    "    ssim = SSIMLoss()\n",
    "\n",
    "    for epoch in range(10):  # quick warmup; adjust via TrainConfig\n",
    "        for batch in dataloader:\n",
    "            for m in modalities:\n",
    "                x = batch[m].to(device)\n",
    "                opts[m].zero_grad(set_to_none=True)\n",
    "                with torch.cuda.amp.autocast(enabled=True):\n",
    "                    y, _ = models[m](x)\n",
    "                    loss = 0.5 * F.l1_loss(y, x) + 0.5 * ssim(y, x)\n",
    "                loss.backward()\n",
    "                opts[m].step()\n",
    "        # (Optional) add validation & early stopping\n",
    "    encoders = {m: models[m].encoder for m in modalities}\n",
    "    return encoders\n",
    "\n",
    "\n",
    "def train_fusion(train_loader: DataLoader, val_loader: Optional[DataLoader], cfg: TrainConfig, device: str = 'cuda') -> nn.Module:\n",
    "    model = FullFusionModel(modalities=cfg.modalities, fusion_mode=cfg.fusion_mode).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)\n",
    "    crit = FusionLoss(LossWeights())\n",
    "\n",
    "    best_val = float('inf')\n",
    "    patience, patience_left = 10, 10\n",
    "\n",
    "    for epoch in range(cfg.epochs_stage23):\n",
    "        model.train()\n",
    "        running = 0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs = {m: batch[m].to(device) for m in cfg.modalities}\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
    "                fused, _, aux = model(inputs)\n",
    "                loss, metrics = crit(fused, inputs, aux)\n",
    "            scaler.scale(loss / cfg.grad_accum).backward()\n",
    "            if (i + 1) % cfg.grad_accum == 0:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            running += loss.item()\n",
    "        # Validation (proxy: loss vs pseudo-target)\n",
    "        val_loss = 0\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    inputs = {m: batch[m].to(device) for m in cfg.modalities}\n",
    "                    fused, _, aux = model(inputs)\n",
    "                    loss, _ = crit(fused, inputs, aux)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= max(1, len(val_loader))\n",
    "            if val_loss < best_val:\n",
    "                best_val = val_loss\n",
    "                patience_left = patience\n",
    "                torch.save(model.state_dict(), 'best_fusion.pt')\n",
    "            else:\n",
    "                patience_left -= 1\n",
    "                if patience_left == 0:\n",
    "                    break\n",
    "        print(f\"Epoch {epoch+1}: train loss {running/len(train_loader):.4f} val {val_loss:.4f}\")\n",
    "\n",
    "    if os.path.exists('best_fusion.pt'):\n",
    "        model.load_state_dict(torch.load('best_fusion.pt', map_location=device))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8397d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Evaluation metrics (no-reference & reference-to-input)\n",
    "# ==========================\n",
    "@torch.no_grad()\n",
    "def entropy(img: torch.Tensor) -> float:\n",
    "    # img: [B,1,H,W] in [0,1]\n",
    "    hist = torch.histc(img, bins=256, min=0.0, max=1.0)\n",
    "    p = hist / hist.sum()\n",
    "    p = p[p>0]\n",
    "    return float(-(p * p.log()).sum().cpu())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def spatial_frequency(img: torch.Tensor) -> float:\n",
    "    dx = F.pad(img, (0,1,0,0))[:, :, :, 1:] - img\n",
    "    dy = F.pad(img, (0,0,0,1))[:, :, 1:, :] - img\n",
    "    rf = torch.sqrt((dx**2).mean() + (dy**2).mean())\n",
    "    return float(rf.cpu())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def mutual_information(img: torch.Tensor, ref: torch.Tensor) -> float:\n",
    "    # crude MI via joint histogram\n",
    "    b = img.shape[0]\n",
    "    mi_vals = []\n",
    "    for i in range(b):\n",
    "        x = img[i].clamp(0,1).flatten()\n",
    "        y = ref[i].clamp(0,1).flatten()\n",
    "        h2d, _, _ = np.histogram2d(x.cpu().numpy(), y.cpu().numpy(), bins=64, range=[[0,1],[0,1]])\n",
    "        pxy = h2d / (h2d.sum() + 1e-6)\n",
    "        px = pxy.sum(axis=1, keepdims=True)\n",
    "        py = pxy.sum(axis=0, keepdims=True)\n",
    "        nz = pxy > 0\n",
    "        mi = (pxy[nz] * (np.log(pxy[nz]) - np.log(px[nz.any(axis=1)]) - np.log(py[:, nz.any(axis=0)]))).sum()\n",
    "        mi_vals.append(mi)\n",
    "    return float(np.mean(mi_vals))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_batch(fused: torch.Tensor, inputs: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
    "    metrics = {\n",
    "        'entropy': entropy(fused),\n",
    "        'spatial_freq': spatial_frequency(fused),\n",
    "    }\n",
    "    # reference-to-input SSIM/MI per modality\n",
    "    ssim = SSIMLoss()\n",
    "    for m, x in inputs.items():\n",
    "        metrics[f'ssim_{m}'] = float(1 - ssim(fused, x).item())\n",
    "        metrics[f'mi_{m}'] = mutual_information(fused, x)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Example usage (pseudo CLI)\n",
    "# ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    seed_all(123)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Example: replace with your dataset listing\n",
    "    pairs = [\n",
    "        # {'ct': 'samples/ct_000.png', 'mri': 'samples/mri_000.png'},\n",
    "    ]\n",
    "    modalities = ['ct', 'mri']\n",
    "\n",
    "    common_tf = transforms.Compose([\n",
    "        # Intensity & geometric aug for small data\n",
    "        transforms.Lambda(lambda x: x + 0.01 * torch.randn_like(x)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomResizedCrop(size=(256, 256), scale=(0.9, 1.0)),\n",
    "    ])\n",
    "\n",
    "    ds = PairedFusionDataset(pairs, modalities, transform=common_tf)\n",
    "    if len(ds) == 0:\n",
    "        print(\"Populate 'pairs' with your image paths to run training.\")\n",
    "        exit(0)\n",
    "\n",
    "    n_val = max(1, int(0.1 * len(ds)))\n",
    "    train_ds, val_ds = torch.utils.data.random_split(ds, [len(ds) - n_val, n_val])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=2)\n",
    "\n",
    "    cfg = TrainConfig(modalities=modalities, fusion_mode=\"concat_conv\", epochs_stage23=50)\n",
    "    model = train_fusion(train_loader, val_loader, cfg, device=device)\n",
    "\n",
    "    # Inference on one batch\n",
    "    batch = next(iter(val_loader))\n",
    "    with torch.no_grad():\n",
    "        fused, _, _ = model({m: batch[m].to(device) for m in modalities})\n",
    "    print(\"Model params:\", num_params(model)/1e6, \"M\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
